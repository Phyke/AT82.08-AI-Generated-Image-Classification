{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274db86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5981a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root/\n",
    "#   train/\n",
    "#     REAL/\n",
    "#     FAKE/\n",
    "#   test/\n",
    "#     REAL/\n",
    "#     FAKE/\n",
    "\n",
    "CIFAKE_ROOT = \"data/\"\n",
    "\n",
    "IMAGE_EXTENSIONS = (\"*.png\", \"*.jpg\", \"*.jpeg\")\n",
    "\n",
    "# LBP params\n",
    "LBP_P = 8\n",
    "LBP_R = 1\n",
    "LBP_METHOD = \"uniform\"\n",
    "\n",
    "# HOG params (for 32x32 images)\n",
    "HOG_ORIENTATIONS = 9\n",
    "HOG_PIXELS_PER_CELL = (8, 8)\n",
    "HOG_CELLS_PER_BLOCK = (2, 2)\n",
    "\n",
    "# Gabor params\n",
    "GABOR_FREQUENCIES = [0.2, 0.4]\n",
    "GABOR_THETAS = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]\n",
    "\n",
    "# Color hist params\n",
    "COLOR_HIST_BINS = 16\n",
    "\n",
    "type GrayImage = NDArray[np.uint8]\n",
    "type ColorImage = NDArray[np.uint8]\n",
    "type FeatureVector = NDArray[np.float32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper: load images & labels\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def load_split(split_dir: str) -> tuple[list[np.ndarray], np.ndarray]:\n",
    "    \"\"\"Load images and labels from split_dir (train or test).\n",
    "\n",
    "    Assumes subfolders 'REAL' and 'FAKE'.\n",
    "    Label mapping: real -> 0, fake -> 1\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label_name, label_value in [(\"REAL\", 0), (\"FAKE\", 1)]:\n",
    "        class_dir = Path(split_dir) / label_name\n",
    "        if not class_dir.is_dir():\n",
    "            msg = f\"Directory not found: {class_dir}\"\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        for ext in IMAGE_EXTENSIONS:\n",
    "            for img_path in list(Path(class_dir).glob(ext))[:100]:\n",
    "                # Load with OpenCV (BGR by default)\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                # Convert BGR to RGB\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                images.append(img)\n",
    "                labels.append(label_value)\n",
    "\n",
    "    return images, np.array(labels)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extractors\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def extract_lbp_histogram(gray: GrayImage) -> FeatureVector:\n",
    "    \"\"\"Extract Local Binary Pattern (LBP) features - proper implementation.\"\"\"\n",
    "    # Manual LBP implementation for P=8, R=1, uniform pattern\n",
    "    h, w = gray.shape\n",
    "    lbp = np.zeros((h - 2, w - 2), dtype=np.uint8)\n",
    "\n",
    "    # 8-neighbor offsets for radius=1\n",
    "    offsets = [(-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1)]\n",
    "\n",
    "    for y in range(1, h - 1):\n",
    "        for x in range(1, w - 1):\n",
    "            center = gray[y, x]\n",
    "            code = 0\n",
    "            for i, (dy, dx) in enumerate(offsets):\n",
    "                if gray[y + dy, x + dx] >= center:\n",
    "                    code |= 1 << i\n",
    "\n",
    "            # Check if uniform (at most 2 transitions)\n",
    "            transitions = bin(code ^ (code >> 1 | (code & 1) << 7)).count(\"1\")\n",
    "            if transitions <= 2:\n",
    "                lbp[y - 1, x - 1] = bin(code).count(\"1\")  # uniform code\n",
    "            else:\n",
    "                lbp[y - 1, x - 1] = LBP_P + 1  # non-uniform\n",
    "\n",
    "    # Compute histogram: uniform patterns have P+2 bins\n",
    "    n_bins = LBP_P + 2\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "\n",
    "def extract_hog_feature(gray: GrayImage) -> FeatureVector:\n",
    "    \"\"\"Extract HOG features using OpenCV to match skimage parameters.\"\"\"\n",
    "    h, w = gray.shape\n",
    "\n",
    "    # Calculate window size based on image dimensions\n",
    "    cell_size = HOG_PIXELS_PER_CELL\n",
    "    block_size = (HOG_CELLS_PER_BLOCK[0] * cell_size[0], HOG_CELLS_PER_BLOCK[1] * cell_size[1])\n",
    "    block_stride = cell_size  # Match skimage default\n",
    "\n",
    "    # Adjust window size to be multiple of cell size\n",
    "    win_w = (w // cell_size[1]) * cell_size[1]\n",
    "    win_h = (h // cell_size[0]) * cell_size[0]\n",
    "\n",
    "    if win_w != w or win_h != h:\n",
    "        gray = cv2.resize(gray, (win_w, win_h))\n",
    "\n",
    "    # Create HOG descriptor\n",
    "    hog = cv2.HOGDescriptor(\n",
    "        _winSize=(win_w, win_h),\n",
    "        _blockSize=block_size,\n",
    "        _blockStride=block_stride,\n",
    "        _cellSize=cell_size,\n",
    "        _nbins=HOG_ORIENTATIONS,\n",
    "    )\n",
    "\n",
    "    # Compute HOG features\n",
    "    features = hog.compute(gray)\n",
    "    return features.flatten().astype(np.float32)\n",
    "\n",
    "\n",
    "def extract_gabor_stats(gray: GrayImage) -> FeatureVector:\n",
    "    \"\"\"Extract Gabor filter statistics matching skimage implementation.\"\"\"\n",
    "    feats = []\n",
    "    gray_float = gray.astype(np.float32) / 255.0\n",
    "\n",
    "    for freq in GABOR_FREQUENCIES:\n",
    "        for theta in GABOR_THETAS:\n",
    "            # Match skimage gabor filter parameters\n",
    "            wavelength = 1.0 / freq\n",
    "            sigma = wavelength / np.pi * np.sqrt(np.log(2) / 2) * (2**1 + 1) / (2**1 - 1)\n",
    "\n",
    "            # Create real and imaginary Gabor kernels\n",
    "            kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
    "            kernel_real = cv2.getGaborKernel(\n",
    "                (kernel_size, kernel_size), sigma, theta, wavelength, 0.5, 0, ktype=cv2.CV_32F\n",
    "            )\n",
    "            kernel_imag = cv2.getGaborKernel(\n",
    "                (kernel_size, kernel_size), sigma, theta, wavelength, 0.5, np.pi / 2, ktype=cv2.CV_32F\n",
    "            )\n",
    "\n",
    "            # Apply filters\n",
    "            real_response = cv2.filter2D(gray_float, cv2.CV_32F, kernel_real)\n",
    "            imag_response = cv2.filter2D(gray_float, cv2.CV_32F, kernel_imag)\n",
    "\n",
    "            # Compute magnitude\n",
    "            mag = np.sqrt(real_response**2 + imag_response**2)\n",
    "\n",
    "            # Compute statistics\n",
    "            feats.append(mag.mean())\n",
    "            feats.append(mag.std())\n",
    "\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "def extract_color_hist(rgb: ColorImage) -> FeatureVector:\n",
    "    \"\"\"Extract per-channel color histogram.\"\"\"\n",
    "    feats = []\n",
    "    for c in range(3):  # R, G, B\n",
    "        hist = cv2.calcHist([rgb], [c], None, [COLOR_HIST_BINS], [0, 256])\n",
    "        hist = hist.flatten()\n",
    "        hist = hist / (hist.sum() + 1e-7)  # normalize\n",
    "        feats.extend(hist)\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main feature extraction\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def compute_feature_groups(images: list[ColorImage]) -> dict[str, NDArray[np.float32]]:\n",
    "    \"\"\"Compute all feature groups for a list of images.\n",
    "\n",
    "    images: list of RGB images\n",
    "    returns: dict(feature_name -> np.array[num_samples, dim])\n",
    "    \"\"\"\n",
    "    feature_names = [\"lbp\", \"hog\", \"gabor\", \"color\"]\n",
    "    feats = {name: [] for name in feature_names}\n",
    "\n",
    "    for img in tqdm(images, desc=\"Extracting features\"):\n",
    "        # Convert to grayscale matching skimage approach\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        feats[\"lbp\"].append(extract_lbp_histogram(gray))\n",
    "        feats[\"hog\"].append(extract_hog_feature(gray))\n",
    "        feats[\"gabor\"].append(extract_gabor_stats(gray))\n",
    "        feats[\"color\"].append(extract_color_hist(img))\n",
    "\n",
    "    # Stack into arrays\n",
    "    for k in feats.keys():\n",
    "        feats[k] = np.vstack(feats[k])\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def concat_feature_groups(feats_dict: dict[str, NDArray[np.float32]], group_names: list[str]) -> NDArray[np.float32]:\n",
    "    \"\"\"Concatenate selected feature groups horizontally.\n",
    "\n",
    "    feats_dict: {name: np.array[num_samples, dim]}\n",
    "    group_names: list of keys to concat\n",
    "    \"\"\"\n",
    "    return np.hstack([feats_dict[name] for name in group_names])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training & evaluation\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    x_train: NDArray[np.float32],\n",
    "    y_train: NDArray[np.int_],\n",
    "    x_test: NDArray[np.float32],\n",
    "    y_test: NDArray[np.int_],\n",
    "    name: str,\n",
    ") -> None:\n",
    "    \"\"\"Train & evaluate logistic regression on given features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Experiment: {name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    clf = Pipeline([(\"scaler\", StandardScaler()), (\"logreg\", LogisticRegression(max_iter=1000, n_jobs=-1))])\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1646e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images...\n",
      "Loading test images...\n",
      "Loading test images...\n",
      "\n",
      "Computing TRAIN features...\n",
      "\n",
      "Computing TRAIN features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 549.28it/s]\n",
      "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 549.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing TEST features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 499.41it/s]\n",
      "Extracting features: 100%|██████████| 200/200 [00:00<00:00, 499.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment: LBP only\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6579    0.7500    0.7009       100\n",
      "           1     0.7093    0.6100    0.6559       100\n",
      "\n",
      "    accuracy                         0.6800       200\n",
      "   macro avg     0.6836    0.6800    0.6784       200\n",
      "weighted avg     0.6836    0.6800    0.6784       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: HOG only\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6579    0.7500    0.7009       100\n",
      "           1     0.7093    0.6100    0.6559       100\n",
      "\n",
      "    accuracy                         0.6800       200\n",
      "   macro avg     0.6836    0.6800    0.6784       200\n",
      "weighted avg     0.6836    0.6800    0.6784       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: HOG only\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6842    0.6500    0.6667       100\n",
      "           1     0.6667    0.7000    0.6829       100\n",
      "\n",
      "    accuracy                         0.6750       200\n",
      "   macro avg     0.6754    0.6750    0.6748       200\n",
      "weighted avg     0.6754    0.6750    0.6748       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: GABOR only\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6842    0.6500    0.6667       100\n",
      "           1     0.6667    0.7000    0.6829       100\n",
      "\n",
      "    accuracy                         0.6750       200\n",
      "   macro avg     0.6754    0.6750    0.6748       200\n",
      "weighted avg     0.6754    0.6750    0.6748       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: GABOR only\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7568    0.8400    0.7962       100\n",
      "           1     0.8202    0.7300    0.7725       100\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.7885    0.7850    0.7843       200\n",
      "weighted avg     0.7885    0.7850    0.7843       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: COLOR only\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7568    0.8400    0.7962       100\n",
      "           1     0.8202    0.7300    0.7725       100\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.7885    0.7850    0.7843       200\n",
      "weighted avg     0.7885    0.7850    0.7843       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: COLOR only\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5980    0.6100    0.6040       100\n",
      "           1     0.6020    0.5900    0.5960       100\n",
      "\n",
      "    accuracy                         0.6000       200\n",
      "   macro avg     0.6000    0.6000    0.6000       200\n",
      "weighted avg     0.6000    0.6000    0.6000       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL: LBP+HOG+GABOR+COLOR\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5980    0.6100    0.6040       100\n",
      "           1     0.6020    0.5900    0.5960       100\n",
      "\n",
      "    accuracy                         0.6000       200\n",
      "   macro avg     0.6000    0.6000    0.6000       200\n",
      "weighted avg     0.6000    0.6000    0.6000       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL: LBP+HOG+GABOR+COLOR\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7957    0.7400    0.7668       100\n",
      "           1     0.7570    0.8100    0.7826       100\n",
      "\n",
      "    accuracy                         0.7750       200\n",
      "   macro avg     0.7764    0.7750    0.7747       200\n",
      "weighted avg     0.7764    0.7750    0.7747       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus LBP (HOG+GABOR+COLOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7957    0.7400    0.7668       100\n",
      "           1     0.7570    0.8100    0.7826       100\n",
      "\n",
      "    accuracy                         0.7750       200\n",
      "   macro avg     0.7764    0.7750    0.7747       200\n",
      "weighted avg     0.7764    0.7750    0.7747       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus LBP (HOG+GABOR+COLOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7526    0.7300    0.7411       100\n",
      "           1     0.7379    0.7600    0.7488       100\n",
      "\n",
      "    accuracy                         0.7450       200\n",
      "   macro avg     0.7452    0.7450    0.7449       200\n",
      "weighted avg     0.7452    0.7450    0.7449       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus HOG (LBP+GABOR+COLOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7526    0.7300    0.7411       100\n",
      "           1     0.7379    0.7600    0.7488       100\n",
      "\n",
      "    accuracy                         0.7450       200\n",
      "   macro avg     0.7452    0.7450    0.7449       200\n",
      "weighted avg     0.7452    0.7450    0.7449       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus HOG (LBP+GABOR+COLOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7383    0.7900    0.7633       100\n",
      "           1     0.7742    0.7200    0.7461       100\n",
      "\n",
      "    accuracy                         0.7550       200\n",
      "   macro avg     0.7563    0.7550    0.7547       200\n",
      "weighted avg     0.7563    0.7550    0.7547       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus GABOR (LBP+HOG+COLOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7383    0.7900    0.7633       100\n",
      "           1     0.7742    0.7200    0.7461       100\n",
      "\n",
      "    accuracy                         0.7550       200\n",
      "   macro avg     0.7563    0.7550    0.7547       200\n",
      "weighted avg     0.7563    0.7550    0.7547       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus GABOR (LBP+HOG+COLOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7582    0.6900    0.7225       100\n",
      "           1     0.7156    0.7800    0.7464       100\n",
      "\n",
      "    accuracy                         0.7350       200\n",
      "   macro avg     0.7369    0.7350    0.7345       200\n",
      "weighted avg     0.7369    0.7350    0.7345       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus COLOR (LBP+HOG+GABOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7582    0.6900    0.7225       100\n",
      "           1     0.7156    0.7800    0.7464       100\n",
      "\n",
      "    accuracy                         0.7350       200\n",
      "   macro avg     0.7369    0.7350    0.7345       200\n",
      "weighted avg     0.7369    0.7350    0.7345       200\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: ALL minus COLOR (LBP+HOG+GABOR)\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8021    0.7700    0.7857       100\n",
      "           1     0.7788    0.8100    0.7941       100\n",
      "\n",
      "    accuracy                         0.7900       200\n",
      "   macro avg     0.7905    0.7900    0.7899       200\n",
      "weighted avg     0.7905    0.7900    0.7899       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8021    0.7700    0.7857       100\n",
      "           1     0.7788    0.8100    0.7941       100\n",
      "\n",
      "    accuracy                         0.7900       200\n",
      "   macro avg     0.7905    0.7900    0.7899       200\n",
      "weighted avg     0.7905    0.7900    0.7899       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data\n",
    "train_dir = Path(CIFAKE_ROOT) / \"train\"\n",
    "test_dir = Path(CIFAKE_ROOT) / \"test\"\n",
    "\n",
    "print(\"Loading training images...\")\n",
    "train_images, y_train = load_split(train_dir)\n",
    "print(\"Loading test images...\")\n",
    "test_images, y_test = load_split(test_dir)\n",
    "\n",
    "# 2. Feature extraction (per group)\n",
    "print(\"\\nComputing TRAIN features...\")\n",
    "train_feats = compute_feature_groups(train_images)\n",
    "print(\"\\nComputing TEST features...\")\n",
    "test_feats = compute_feature_groups(test_images)\n",
    "\n",
    "feature_groups = [\"lbp\", \"hog\", \"gabor\", \"color\"]\n",
    "\n",
    "# 3. Single-feature experiments (each feature alone)\n",
    "for fg in feature_groups:\n",
    "    X_tr = train_feats[fg]\n",
    "    X_te = test_feats[fg]\n",
    "    run_experiment(X_tr, y_train, X_te, y_test, name=f\"{fg.upper()} only\")\n",
    "\n",
    "# 4. All features combined\n",
    "all_name = \"+\".join(fg.upper() for fg in feature_groups)\n",
    "X_train_all = concat_feature_groups(train_feats, feature_groups)\n",
    "X_test_all = concat_feature_groups(test_feats, feature_groups)\n",
    "run_experiment(X_train_all, y_train, X_test_all, y_test, name=f\"ALL: {all_name}\")\n",
    "\n",
    "# 5. Ablation: ALL minus one group\n",
    "for drop_fg in feature_groups:\n",
    "    keep_groups = [fg for fg in feature_groups if fg != drop_fg]\n",
    "    name = f\"ALL minus {drop_fg.upper()} ({'+'.join(kg.upper() for kg in keep_groups)})\"\n",
    "    X_tr = concat_feature_groups(train_feats, keep_groups)\n",
    "    X_te = concat_feature_groups(test_feats, keep_groups)\n",
    "    run_experiment(X_tr, y_train, X_te, y_test, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83e1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize Gabor kernels\n",
    "plt.figure(figsize=(12, 6))\n",
    "i = 1\n",
    "\n",
    "for f in GABOR_FREQUENCIES:\n",
    "    wavelength = 1.0 / f\n",
    "    sigma = wavelength / np.pi * np.sqrt(np.log(2) / 2) * (2**1 + 1) / (2**1 - 1)\n",
    "    kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
    "\n",
    "    for t in GABOR_THETAS:\n",
    "        kernel = cv2.getGaborKernel((kernel_size, kernel_size), sigma, t, wavelength, 0.5, 0, ktype=cv2.CV_32F)\n",
    "        plt.subplot(len(GABOR_FREQUENCIES), len(GABOR_THETAS), i)\n",
    "        plt.imshow(kernel, cmap=\"gray\")\n",
    "        plt.title(f\"f:{f:.1f}, θ:{t:.2f}\")\n",
    "        plt.axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "at82-08-ai-generated-image-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
