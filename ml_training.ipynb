{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b354fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils.ml_training import run_ml_experiments, shuffle_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b2da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbp_train = pd.read_csv(\"data/lbp_features_train.csv\", dtype=\"float32\")\n",
    "df_lbp_test = pd.read_csv(\"data/lbp_features_test.csv\", dtype=\"float32\")\n",
    "df_lbp_train[\"label\"] = df_lbp_train[\"label\"].astype(\"uint8\")\n",
    "df_lbp_test[\"label\"] = df_lbp_test[\"label\"].astype(\"uint8\")\n",
    "df_hog_train = pd.read_csv(\"data/hog_features_train.csv\", dtype=\"float32\")\n",
    "df_hog_test = pd.read_csv(\"data/hog_features_test.csv\", dtype=\"float32\")\n",
    "df_hog_train[\"label\"] = df_hog_train[\"label\"].astype(\"uint8\")\n",
    "df_hog_test[\"label\"] = df_hog_test[\"label\"].astype(\"uint8\")\n",
    "df_gabor_train = pd.read_csv(\"data/gabor_features_train.csv\", dtype=\"float32\")\n",
    "df_gabor_test = pd.read_csv(\"data/gabor_features_test.csv\", dtype=\"float32\")\n",
    "df_gabor_train[\"label\"] = df_gabor_train[\"label\"].astype(\"uint8\")\n",
    "df_gabor_test[\"label\"] = df_gabor_test[\"label\"].astype(\"uint8\")\n",
    "df_color_train = pd.read_csv(\"data/color_features_train.csv\", dtype=\"float32\")\n",
    "df_color_test = pd.read_csv(\"data/color_features_test.csv\", dtype=\"float32\")\n",
    "df_color_train[\"label\"] = df_color_train[\"label\"].astype(\"uint8\")\n",
    "df_color_test[\"label\"] = df_color_test[\"label\"].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb344049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_lbp_train[\"label\"].equals(df_hog_train[\"label\"])\n",
    "    and df_lbp_train[\"label\"].equals(df_gabor_train[\"label\"])\n",
    "    and df_lbp_train[\"label\"].equals(df_color_train[\"label\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b299fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0  0.070312  0.095703  0.058594  0.111328  0.200195  0.151367  0.059570   \n",
       "1  0.062500  0.091797  0.062500  0.105469  0.244141  0.124023  0.046875   \n",
       "2  0.068359  0.095703  0.064453  0.126953  0.177734  0.144531  0.076172   \n",
       "3  0.048828  0.084961  0.065430  0.096680  0.247070  0.145508  0.061523   \n",
       "4  0.065430  0.103516  0.081055  0.110352  0.171875  0.118164  0.093750   \n",
       "\n",
       "      LBP_7     LBP_8     LBP_9  label  \n",
       "0  0.070312  0.050781  0.131836      0  \n",
       "1  0.073242  0.059570  0.129883      0  \n",
       "2  0.066406  0.054688  0.125000      0  \n",
       "3  0.075195  0.069336  0.105469      0  \n",
       "4  0.064453  0.060547  0.130859      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df7b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.092773</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0  0.074219  0.099609  0.061523  0.117188  0.161133  0.127930  0.076172   \n",
       "1  0.075195  0.092773  0.057617  0.117188  0.190430  0.128906  0.072266   \n",
       "2  0.067383  0.082031  0.067383  0.130859  0.201172  0.138672  0.073242   \n",
       "3  0.063477  0.105469  0.073242  0.111328  0.190430  0.112305  0.050781   \n",
       "4  0.039062  0.086914  0.069336  0.161133  0.194336  0.159180  0.078125   \n",
       "\n",
       "      LBP_7     LBP_8     LBP_9  label  \n",
       "0  0.076172  0.058594  0.147461      0  \n",
       "1  0.062500  0.067383  0.135742      0  \n",
       "2  0.068359  0.050781  0.120117      0  \n",
       "3  0.090820  0.069336  0.132812      0  \n",
       "4  0.071289  0.049805  0.090820      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d746c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOG_0</th>\n",
       "      <th>HOG_1</th>\n",
       "      <th>HOG_2</th>\n",
       "      <th>HOG_3</th>\n",
       "      <th>HOG_4</th>\n",
       "      <th>HOG_5</th>\n",
       "      <th>HOG_6</th>\n",
       "      <th>HOG_7</th>\n",
       "      <th>HOG_8</th>\n",
       "      <th>HOG_9</th>\n",
       "      <th>...</th>\n",
       "      <th>HOG_315</th>\n",
       "      <th>HOG_316</th>\n",
       "      <th>HOG_317</th>\n",
       "      <th>HOG_318</th>\n",
       "      <th>HOG_319</th>\n",
       "      <th>HOG_320</th>\n",
       "      <th>HOG_321</th>\n",
       "      <th>HOG_322</th>\n",
       "      <th>HOG_323</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.164556</td>\n",
       "      <td>0.110298</td>\n",
       "      <td>0.114931</td>\n",
       "      <td>0.271179</td>\n",
       "      <td>0.169717</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>0.032058</td>\n",
       "      <td>0.128274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178965</td>\n",
       "      <td>0.071544</td>\n",
       "      <td>0.034731</td>\n",
       "      <td>0.160832</td>\n",
       "      <td>0.215133</td>\n",
       "      <td>0.077233</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.174348</td>\n",
       "      <td>0.053133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042332</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>0.186208</td>\n",
       "      <td>0.458941</td>\n",
       "      <td>0.127396</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>0.044536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.058476</td>\n",
       "      <td>0.338194</td>\n",
       "      <td>0.090360</td>\n",
       "      <td>0.037042</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171644</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.085235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293204</td>\n",
       "      <td>0.312434</td>\n",
       "      <td>0.416592</td>\n",
       "      <td>0.135259</td>\n",
       "      <td>0.200463</td>\n",
       "      <td>0.170252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116966</td>\n",
       "      <td>0.040826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157485</td>\n",
       "      <td>0.174811</td>\n",
       "      <td>0.229987</td>\n",
       "      <td>0.108690</td>\n",
       "      <td>0.194863</td>\n",
       "      <td>0.189121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039423</td>\n",
       "      <td>0.076545</td>\n",
       "      <td>0.318370</td>\n",
       "      <td>0.238697</td>\n",
       "      <td>0.182499</td>\n",
       "      <td>0.100955</td>\n",
       "      <td>0.084661</td>\n",
       "      <td>0.228457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>0.154246</td>\n",
       "      <td>0.773984</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.052806</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.244265</td>\n",
       "      <td>0.088502</td>\n",
       "      <td>0.200047</td>\n",
       "      <td>0.125870</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154710</td>\n",
       "      <td>0.076618</td>\n",
       "      <td>0.048245</td>\n",
       "      <td>0.139931</td>\n",
       "      <td>0.181676</td>\n",
       "      <td>0.019553</td>\n",
       "      <td>0.061611</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>0.039013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HOG_0     HOG_1     HOG_2     HOG_3     HOG_4     HOG_5     HOG_6  \\\n",
       "0  0.164556  0.110298  0.114931  0.271179  0.169717  0.054026  0.000000   \n",
       "1  0.042332  0.014491  0.008546  0.186208  0.458941  0.127396  0.026936   \n",
       "2  0.171644  0.030002  0.085235  0.000000  0.293204  0.312434  0.416592   \n",
       "3  0.137398  0.000000  0.039423  0.076545  0.318370  0.238697  0.182499   \n",
       "4  0.052806  0.023038  0.244265  0.088502  0.200047  0.125870  0.030002   \n",
       "\n",
       "      HOG_7     HOG_8     HOG_9  ...   HOG_315   HOG_316   HOG_317   HOG_318  \\\n",
       "0  0.013909  0.032058  0.128274  ...  0.178965  0.071544  0.034731  0.160832   \n",
       "1  0.002414  0.010556  0.044536  ...  0.016068  0.005337  0.001035  0.058476   \n",
       "2  0.135259  0.200463  0.170252  ...  0.116966  0.040826  0.000000  0.157485   \n",
       "3  0.100955  0.084661  0.228457  ...  0.025416  0.000000  0.005985  0.154246   \n",
       "4  0.026564  0.000000  0.116017  ...  0.154710  0.076618  0.048245  0.139931   \n",
       "\n",
       "    HOG_319   HOG_320   HOG_321   HOG_322   HOG_323  label  \n",
       "0  0.215133  0.077233  0.080584  0.174348  0.053133      0  \n",
       "1  0.338194  0.090360  0.037042  0.018520  0.009288      0  \n",
       "2  0.174811  0.229987  0.108690  0.194863  0.189121      0  \n",
       "3  0.773984  0.022197  0.002287  0.006587  0.000000      0  \n",
       "4  0.181676  0.019553  0.061611  0.012055  0.039013      0  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hog_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc041b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOG_0</th>\n",
       "      <th>HOG_1</th>\n",
       "      <th>HOG_2</th>\n",
       "      <th>HOG_3</th>\n",
       "      <th>HOG_4</th>\n",
       "      <th>HOG_5</th>\n",
       "      <th>HOG_6</th>\n",
       "      <th>HOG_7</th>\n",
       "      <th>HOG_8</th>\n",
       "      <th>HOG_9</th>\n",
       "      <th>...</th>\n",
       "      <th>HOG_315</th>\n",
       "      <th>HOG_316</th>\n",
       "      <th>HOG_317</th>\n",
       "      <th>HOG_318</th>\n",
       "      <th>HOG_319</th>\n",
       "      <th>HOG_320</th>\n",
       "      <th>HOG_321</th>\n",
       "      <th>HOG_322</th>\n",
       "      <th>HOG_323</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.133233</td>\n",
       "      <td>0.147257</td>\n",
       "      <td>0.155707</td>\n",
       "      <td>0.247196</td>\n",
       "      <td>0.234899</td>\n",
       "      <td>0.161379</td>\n",
       "      <td>0.039895</td>\n",
       "      <td>0.113214</td>\n",
       "      <td>0.084901</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221702</td>\n",
       "      <td>0.220815</td>\n",
       "      <td>0.207102</td>\n",
       "      <td>0.148685</td>\n",
       "      <td>0.139678</td>\n",
       "      <td>0.046422</td>\n",
       "      <td>0.103002</td>\n",
       "      <td>0.022021</td>\n",
       "      <td>0.095391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.035142</td>\n",
       "      <td>0.322201</td>\n",
       "      <td>0.252297</td>\n",
       "      <td>0.047138</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.064328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064858</td>\n",
       "      <td>0.141359</td>\n",
       "      <td>0.082815</td>\n",
       "      <td>0.041605</td>\n",
       "      <td>0.027038</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.108954</td>\n",
       "      <td>0.021148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102442</td>\n",
       "      <td>0.017749</td>\n",
       "      <td>0.145625</td>\n",
       "      <td>0.067538</td>\n",
       "      <td>0.116081</td>\n",
       "      <td>0.014466</td>\n",
       "      <td>0.020879</td>\n",
       "      <td>0.016245</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162728</td>\n",
       "      <td>0.056431</td>\n",
       "      <td>0.104525</td>\n",
       "      <td>0.113858</td>\n",
       "      <td>0.246311</td>\n",
       "      <td>0.130088</td>\n",
       "      <td>0.113036</td>\n",
       "      <td>0.048274</td>\n",
       "      <td>0.146711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.126457</td>\n",
       "      <td>0.061866</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.056195</td>\n",
       "      <td>0.104592</td>\n",
       "      <td>0.047203</td>\n",
       "      <td>0.015626</td>\n",
       "      <td>0.116695</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.178866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227151</td>\n",
       "      <td>0.197967</td>\n",
       "      <td>0.370338</td>\n",
       "      <td>0.180816</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.047681</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.114480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.198966</td>\n",
       "      <td>0.074091</td>\n",
       "      <td>0.031344</td>\n",
       "      <td>0.263441</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.199880</td>\n",
       "      <td>0.073618</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>0.069013</td>\n",
       "      <td>0.186227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147104</td>\n",
       "      <td>0.090647</td>\n",
       "      <td>0.060828</td>\n",
       "      <td>0.107004</td>\n",
       "      <td>0.084967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>0.015751</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HOG_0     HOG_1     HOG_2     HOG_3     HOG_4     HOG_5     HOG_6  \\\n",
       "0  0.133233  0.147257  0.155707  0.247196  0.234899  0.161379  0.039895   \n",
       "1  0.059590  0.010019  0.035142  0.322201  0.252297  0.047138  0.013749   \n",
       "2  0.102442  0.017749  0.145625  0.067538  0.116081  0.014466  0.020879   \n",
       "3  0.126457  0.061866  0.000678  0.056195  0.104592  0.047203  0.015626   \n",
       "4  0.198966  0.074091  0.031344  0.263441  0.384085  0.199880  0.073618   \n",
       "\n",
       "      HOG_7     HOG_8     HOG_9  ...   HOG_315   HOG_316   HOG_317   HOG_318  \\\n",
       "0  0.113214  0.084901  0.012819  ...  0.221702  0.220815  0.207102  0.148685   \n",
       "1  0.005209  0.004802  0.064328  ...  0.064858  0.141359  0.082815  0.041605   \n",
       "2  0.016245  0.017870  0.038912  ...  0.162728  0.056431  0.104525  0.113858   \n",
       "3  0.116695  0.055012  0.178866  ...  0.227151  0.197967  0.370338  0.180816   \n",
       "4  0.047190  0.069013  0.186227  ...  0.147104  0.090647  0.060828  0.107004   \n",
       "\n",
       "    HOG_319   HOG_320   HOG_321   HOG_322   HOG_323  label  \n",
       "0  0.139678  0.046422  0.103002  0.022021  0.095391      0  \n",
       "1  0.027038  0.058822  0.060822  0.108954  0.021148      0  \n",
       "2  0.246311  0.130088  0.113036  0.048274  0.146711      0  \n",
       "3  0.171524  0.047681  0.013201  0.034434  0.114480      0  \n",
       "4  0.084967  0.000000  0.016345  0.015751  0.055143      0  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hog_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a225435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.035235</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.017255</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010986</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.012773</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.013296</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016285</td>\n",
       "      <td>0.012491</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.017579</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.011420</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010716</td>\n",
       "      <td>0.013054</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0            0.025487           0.015180             0.015745   \n",
       "1            0.010986           0.012186             0.012773   \n",
       "2            0.016285           0.012491             0.013393   \n",
       "3            0.010716           0.013054             0.010267   \n",
       "4            0.010842           0.008827             0.007563   \n",
       "\n",
       "   gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0            0.008965             0.035235            0.023189   \n",
       "1            0.015618             0.022050            0.013359   \n",
       "2            0.008042             0.017579            0.009739   \n",
       "3            0.008752             0.022262            0.023039   \n",
       "4            0.004795             0.013905            0.008357   \n",
       "\n",
       "   gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0              0.017233             0.009223            0.014462   \n",
       "1              0.015124             0.013296            0.008714   \n",
       "2              0.013057             0.007870            0.011420   \n",
       "3              0.009274             0.010089            0.005055   \n",
       "4              0.008254             0.007586            0.009359   \n",
       "\n",
       "   gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0           0.010695             0.010592            0.006646   \n",
       "1           0.012276             0.006274            0.007100   \n",
       "2           0.009930             0.008178            0.004998   \n",
       "3           0.005926             0.004737            0.004743   \n",
       "4           0.007253             0.006268            0.003545   \n",
       "\n",
       "   gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0             0.021182            0.017255              0.012010   \n",
       "1             0.011564            0.008165              0.009103   \n",
       "2             0.009171            0.005822              0.009335   \n",
       "3             0.010330            0.011244              0.004923   \n",
       "4             0.006657            0.004499              0.005974   \n",
       "\n",
       "   gabor_f0.4_t135_std  label  \n",
       "0             0.007830      0  \n",
       "1             0.009235      0  \n",
       "2             0.006005      0  \n",
       "3             0.005105      0  \n",
       "4             0.003613      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gabor_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfafde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.027085</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.015226</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>0.015012</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016755</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.020906</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.015775</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>0.012243</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>0.007593</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0            0.019114           0.011690             0.013208   \n",
       "1            0.016755           0.014683             0.020438   \n",
       "2            0.021395           0.015775             0.009892   \n",
       "3            0.018256           0.014299             0.014059   \n",
       "4            0.004899           0.003762             0.004362   \n",
       "\n",
       "   gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0            0.008110             0.027085            0.017980   \n",
       "1            0.015542             0.024632            0.020906   \n",
       "2            0.007718             0.008575            0.008443   \n",
       "3            0.010218             0.012424            0.008191   \n",
       "4            0.003657             0.006170            0.004497   \n",
       "\n",
       "   gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0              0.014299             0.007634            0.015226   \n",
       "1              0.014697             0.015717            0.010085   \n",
       "2              0.010142             0.007819            0.012059   \n",
       "3              0.012866             0.009325            0.012865   \n",
       "4              0.004168             0.003116            0.004336   \n",
       "\n",
       "   gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0           0.013386             0.010334            0.007067   \n",
       "1           0.010674             0.010974            0.010191   \n",
       "2           0.012243             0.006099            0.005641   \n",
       "3           0.012881             0.008574            0.006384   \n",
       "4           0.003628             0.002837            0.002456   \n",
       "\n",
       "   gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0             0.018516            0.015012              0.009861   \n",
       "1             0.011706            0.012159              0.008319   \n",
       "2             0.005278            0.004273              0.005680   \n",
       "3             0.007717            0.007593              0.009149   \n",
       "4             0.002710            0.002006              0.002680   \n",
       "\n",
       "   gabor_f0.4_t135_std  label  \n",
       "0             0.006252      0  \n",
       "1             0.007945      0  \n",
       "2             0.005195      0  \n",
       "3             0.007213      0  \n",
       "4             0.002148      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gabor_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f17198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_hist_0</th>\n",
       "      <th>red_hist_1</th>\n",
       "      <th>red_hist_2</th>\n",
       "      <th>red_hist_3</th>\n",
       "      <th>red_hist_4</th>\n",
       "      <th>red_hist_5</th>\n",
       "      <th>red_hist_6</th>\n",
       "      <th>red_hist_7</th>\n",
       "      <th>red_hist_8</th>\n",
       "      <th>red_hist_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.008456</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.018566</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_hist_0  red_hist_1  red_hist_2  red_hist_3  red_hist_4  red_hist_5  \\\n",
       "0    0.000919    0.001042    0.003493    0.003799    0.005576    0.007292   \n",
       "1    0.004044    0.003309    0.004902    0.009681    0.011213    0.008150   \n",
       "2    0.000245    0.000919    0.001716    0.003248    0.004963    0.007475   \n",
       "3    0.000613    0.018566    0.007966    0.006005    0.004534    0.003676   \n",
       "4    0.000000    0.000306    0.002512    0.003370    0.009191    0.018934   \n",
       "\n",
       "   red_hist_6  red_hist_7  red_hist_8  red_hist_9  ...  hsv_h7_s1_v3  \\\n",
       "0    0.005208    0.004657    0.004779    0.004596  ...           0.0   \n",
       "1    0.003248    0.001961    0.002145    0.003125  ...           0.0   \n",
       "2    0.008578    0.007475    0.008456    0.006495  ...           0.0   \n",
       "3    0.003922    0.004534    0.002757    0.001593  ...           0.0   \n",
       "4    0.012255    0.007475    0.002880    0.001103  ...           0.0   \n",
       "\n",
       "   hsv_h7_s2_v0  hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  \\\n",
       "0      0.000000           0.0           0.0           0.0      0.000000   \n",
       "1      0.000977           0.0           0.0           0.0      0.001953   \n",
       "2      0.000000           0.0           0.0           0.0      0.000000   \n",
       "3      0.000000           0.0           0.0           0.0      0.000000   \n",
       "4      0.000000           0.0           0.0           0.0      0.000000   \n",
       "\n",
       "   hsv_h7_s3_v1  hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0           0.0           0.0           0.0      0  \n",
       "1           0.0           0.0           0.0      0  \n",
       "2           0.0           0.0           0.0      0  \n",
       "3           0.0           0.0           0.0      0  \n",
       "4           0.0           0.0           0.0      0  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_color_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af23bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_hist_0</th>\n",
       "      <th>red_hist_1</th>\n",
       "      <th>red_hist_2</th>\n",
       "      <th>red_hist_3</th>\n",
       "      <th>red_hist_4</th>\n",
       "      <th>red_hist_5</th>\n",
       "      <th>red_hist_6</th>\n",
       "      <th>red_hist_7</th>\n",
       "      <th>red_hist_8</th>\n",
       "      <th>red_hist_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.008885</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.024449</td>\n",
       "      <td>0.027390</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_hist_0  red_hist_1  red_hist_2  red_hist_3  red_hist_4  red_hist_5  \\\n",
       "0    0.000429    0.003431    0.003248    0.003554    0.004105    0.002574   \n",
       "1    0.005944    0.004534    0.004105    0.003799    0.004779    0.003676   \n",
       "2    0.001103    0.003983    0.004963    0.005331    0.003248    0.003186   \n",
       "3    0.000123    0.001716    0.006556    0.007598    0.004657    0.005331   \n",
       "4    0.000000    0.001164    0.024449    0.027390    0.006985    0.001961   \n",
       "\n",
       "   red_hist_6  red_hist_7  red_hist_8  red_hist_9  ...  hsv_h7_s1_v3  \\\n",
       "0    0.002757    0.004657    0.006066    0.007292  ...      0.000000   \n",
       "1    0.004167    0.004534    0.008885    0.006311  ...      0.000977   \n",
       "2    0.005086    0.011458    0.017279    0.005025  ...      0.000000   \n",
       "3    0.005453    0.006556    0.006127    0.007169  ...      0.000000   \n",
       "4    0.000613    0.000184    0.000000    0.000000  ...      0.000000   \n",
       "\n",
       "   hsv_h7_s2_v0  hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  \\\n",
       "0      0.000000      0.000000      0.000000           0.0           0.0   \n",
       "1      0.007812      0.018555      0.003906           0.0           0.0   \n",
       "2      0.006836      0.000977      0.000000           0.0           0.0   \n",
       "3      0.000000      0.000000      0.000000           0.0           0.0   \n",
       "4      0.000000      0.000000      0.000000           0.0           0.0   \n",
       "\n",
       "   hsv_h7_s3_v1  hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0      0.000000      0.000000           0.0      0  \n",
       "1      0.011719      0.003906           0.0      0  \n",
       "2      0.000000      0.000000           0.0      0  \n",
       "3      0.000000      0.000000           0.0      0  \n",
       "4      0.000000      0.000000           0.0      0  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_color_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2dfe503",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = pd.concat(\n",
    "    [\n",
    "        df_lbp_train.drop(columns=[\"label\"]),\n",
    "        df_hog_train.drop(columns=[\"label\"]),\n",
    "        df_gabor_train.drop(columns=[\"label\"]),\n",
    "        df_color_train.drop(columns=[\"label\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df_all_train[\"label\"] = df_lbp_train[\"label\"]\n",
    "df_all_test = pd.concat(\n",
    "    [\n",
    "        df_lbp_test.drop(columns=[\"label\"]),\n",
    "        df_hog_test.drop(columns=[\"label\"]),\n",
    "        df_gabor_test.drop(columns=[\"label\"]),\n",
    "        df_color_test.drop(columns=[\"label\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df_all_test[\"label\"] = df_lbp_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1838a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "LBP_ONLY_COLS = [col for col in df_lbp_train.columns if col != \"label\"]\n",
    "HOG_ONLY_COLS = [col for col in df_hog_train.columns if col != \"label\"]\n",
    "GABOR_ONLY_COLS = [col for col in df_gabor_train.columns if col != \"label\"]\n",
    "COLOR_ONLY_COLS = [col for col in df_color_train.columns if col != \"label\"]\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"LBP\": LBP_ONLY_COLS,\n",
    "    \"HOG\": HOG_ONLY_COLS,\n",
    "    \"Gabor\": GABOR_ONLY_COLS,\n",
    "    \"Color\": COLOR_ONLY_COLS,\n",
    "    \"All\": df_all_train.columns.drop(\"label\").tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114a5c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0.276367</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0      0.070312  0.095703  0.058594  0.111328  0.200195  0.151367  0.059570   \n",
       "1      0.062500  0.091797  0.062500  0.105469  0.244141  0.124023  0.046875   \n",
       "2      0.068359  0.095703  0.064453  0.126953  0.177734  0.144531  0.076172   \n",
       "3      0.048828  0.084961  0.065430  0.096680  0.247070  0.145508  0.061523   \n",
       "4      0.065430  0.103516  0.081055  0.110352  0.171875  0.118164  0.093750   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  0.053711  0.096680  0.051758  0.131836  0.244141  0.147461  0.052734   \n",
       "99996  0.041016  0.056641  0.041992  0.122070  0.276367  0.180664  0.063477   \n",
       "99997  0.068359  0.113281  0.055664  0.085938  0.205078  0.113281  0.045898   \n",
       "99998  0.044922  0.119141  0.058594  0.124023  0.193359  0.126953  0.064453   \n",
       "99999  0.043945  0.077148  0.040039  0.096680  0.174805  0.214844  0.079102   \n",
       "\n",
       "          LBP_7     LBP_8     LBP_9  ...  hsv_h7_s1_v3  hsv_h7_s2_v0  \\\n",
       "0      0.070312  0.050781  0.131836  ...           0.0      0.000000   \n",
       "1      0.073242  0.059570  0.129883  ...           0.0      0.000977   \n",
       "2      0.066406  0.054688  0.125000  ...           0.0      0.000000   \n",
       "3      0.075195  0.069336  0.105469  ...           0.0      0.000000   \n",
       "4      0.064453  0.060547  0.130859  ...           0.0      0.000000   \n",
       "...         ...       ...       ...  ...           ...           ...   \n",
       "99995  0.069336  0.053711  0.098633  ...           0.0      0.000977   \n",
       "99996  0.054688  0.068359  0.094727  ...           0.0      0.000977   \n",
       "99997  0.086914  0.088867  0.136719  ...           0.0      0.006836   \n",
       "99998  0.093750  0.050781  0.124023  ...           0.0      0.000000   \n",
       "99999  0.091797  0.083984  0.097656  ...           0.0      0.003906   \n",
       "\n",
       "       hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  hsv_h7_s3_v1  \\\n",
       "0               0.0           0.0           0.0      0.000000           0.0   \n",
       "1               0.0           0.0           0.0      0.001953           0.0   \n",
       "2               0.0           0.0           0.0      0.000000           0.0   \n",
       "3               0.0           0.0           0.0      0.000000           0.0   \n",
       "4               0.0           0.0           0.0      0.000000           0.0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "99995           0.0           0.0           0.0      0.001953           0.0   \n",
       "99996           0.0           0.0           0.0      0.000977           0.0   \n",
       "99997           0.0           0.0           0.0      0.002930           0.0   \n",
       "99998           0.0           0.0           0.0      0.000000           0.0   \n",
       "99999           0.0           0.0           0.0      0.000977           0.0   \n",
       "\n",
       "       hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0               0.0           0.0      0  \n",
       "1               0.0           0.0      0  \n",
       "2               0.0           0.0      0  \n",
       "3               0.0           0.0      0  \n",
       "4               0.0           0.0      0  \n",
       "...             ...           ...    ...  \n",
       "99995           0.0           0.0      1  \n",
       "99996           0.0           0.0      1  \n",
       "99997           0.0           0.0      1  \n",
       "99998           0.0           0.0      1  \n",
       "99999           0.0           0.0      1  \n",
       "\n",
       "[100000 rows x 527 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52527cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.035235</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.017255</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.007830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010986</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.012773</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.013296</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.009235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016285</td>\n",
       "      <td>0.012491</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.017579</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.011420</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.006005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010716</td>\n",
       "      <td>0.013054</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.005105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.003613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.023709</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>0.010362</td>\n",
       "      <td>0.018512</td>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>0.013666</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>0.015813</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.009398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>0.010681</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.014681</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.009966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.016462</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.017052</td>\n",
       "      <td>0.011470</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>0.008679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.019635</td>\n",
       "      <td>0.024341</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.017972</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>0.009530</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>0.025456</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.006596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.025979</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.020543</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.008676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0                0.025487           0.015180             0.015745   \n",
       "1                0.010986           0.012186             0.012773   \n",
       "2                0.016285           0.012491             0.013393   \n",
       "3                0.010716           0.013054             0.010267   \n",
       "4                0.010842           0.008827             0.007563   \n",
       "...                   ...                ...                  ...   \n",
       "99995            0.025049           0.023709             0.010386   \n",
       "99996            0.007965           0.009670             0.008950   \n",
       "99997            0.022883           0.021229             0.016462   \n",
       "99998            0.019635           0.024341             0.007484   \n",
       "99999            0.004822           0.007053             0.009022   \n",
       "\n",
       "       gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0                0.008965             0.035235            0.023189   \n",
       "1                0.015618             0.022050            0.013359   \n",
       "2                0.008042             0.017579            0.009739   \n",
       "3                0.008752             0.022262            0.023039   \n",
       "4                0.004795             0.013905            0.008357   \n",
       "...                   ...                  ...                 ...   \n",
       "99995            0.010362             0.018512            0.016688   \n",
       "99996            0.012678             0.008978            0.010681   \n",
       "99997            0.017172             0.017052            0.011470   \n",
       "99998            0.009505             0.017972            0.023331   \n",
       "99999            0.013302             0.016100            0.025979   \n",
       "\n",
       "       gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0                  0.017233             0.009223            0.014462   \n",
       "1                  0.015124             0.013296            0.008714   \n",
       "2                  0.013057             0.007870            0.011420   \n",
       "3                  0.009274             0.010089            0.005055   \n",
       "4                  0.008254             0.007586            0.009359   \n",
       "...                     ...                  ...                 ...   \n",
       "99995              0.012819             0.014602            0.014546   \n",
       "99996              0.011538             0.014681            0.004960   \n",
       "99997              0.013510             0.011879            0.013327   \n",
       "99998              0.009134             0.009530            0.010233   \n",
       "99999              0.004746             0.006832            0.006472   \n",
       "\n",
       "       gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0               0.010695             0.010592            0.006646   \n",
       "1               0.012276             0.006274            0.007100   \n",
       "2               0.009930             0.008178            0.004998   \n",
       "3               0.005926             0.004737            0.004743   \n",
       "4               0.007253             0.006268            0.003545   \n",
       "...                  ...                  ...                 ...   \n",
       "99995           0.013666             0.006869            0.006157   \n",
       "99996           0.007368             0.005335            0.007723   \n",
       "99997           0.015917             0.010689            0.014874   \n",
       "99998           0.013153             0.004244            0.005433   \n",
       "99999           0.012898             0.006458            0.010627   \n",
       "\n",
       "       gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0                 0.021182            0.017255              0.012010   \n",
       "1                 0.011564            0.008165              0.009103   \n",
       "2                 0.009171            0.005822              0.009335   \n",
       "3                 0.010330            0.011244              0.004923   \n",
       "4                 0.006657            0.004499              0.005974   \n",
       "...                    ...                 ...                   ...   \n",
       "99995             0.013893            0.015813              0.008002   \n",
       "99996             0.007333            0.011752              0.006975   \n",
       "99997             0.013905            0.009374              0.008678   \n",
       "99998             0.016788            0.025456              0.005581   \n",
       "99999             0.011596            0.020543              0.005488   \n",
       "\n",
       "       gabor_f0.4_t135_std  \n",
       "0                 0.007830  \n",
       "1                 0.009235  \n",
       "2                 0.006005  \n",
       "3                 0.005105  \n",
       "4                 0.003613  \n",
       "...                    ...  \n",
       "99995             0.009398  \n",
       "99996             0.009966  \n",
       "99997             0.008679  \n",
       "99998             0.006596  \n",
       "99999             0.008676  \n",
       "\n",
       "[100000 rows x 16 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train[GABOR_ONLY_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b808ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.092773</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0      0.074219  0.099609  0.061523  0.117188  0.161133  0.127930  0.076172   \n",
       "1      0.075195  0.092773  0.057617  0.117188  0.190430  0.128906  0.072266   \n",
       "2      0.067383  0.082031  0.067383  0.130859  0.201172  0.138672  0.073242   \n",
       "3      0.063477  0.105469  0.073242  0.111328  0.190430  0.112305  0.050781   \n",
       "4      0.039062  0.086914  0.069336  0.161133  0.194336  0.159180  0.078125   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995  0.069336  0.099609  0.061523  0.100586  0.199219  0.133789  0.052734   \n",
       "19996  0.078125  0.111328  0.055664  0.109375  0.181641  0.109375  0.060547   \n",
       "19997  0.062500  0.076172  0.083984  0.127930  0.237305  0.129883  0.055664   \n",
       "19998  0.043945  0.094727  0.071289  0.106445  0.246094  0.154297  0.056641   \n",
       "19999  0.043945  0.100586  0.039062  0.094727  0.246094  0.213867  0.036133   \n",
       "\n",
       "          LBP_7     LBP_8     LBP_9  ...  hsv_h7_s1_v3  hsv_h7_s2_v0  \\\n",
       "0      0.076172  0.058594  0.147461  ...      0.000000      0.000000   \n",
       "1      0.062500  0.067383  0.135742  ...      0.000977      0.007812   \n",
       "2      0.068359  0.050781  0.120117  ...      0.000000      0.006836   \n",
       "3      0.090820  0.069336  0.132812  ...      0.000000      0.000000   \n",
       "4      0.071289  0.049805  0.090820  ...      0.000000      0.000000   \n",
       "...         ...       ...       ...  ...           ...           ...   \n",
       "19995  0.083008  0.061523  0.138672  ...      0.000000      0.000000   \n",
       "19996  0.085938  0.074219  0.133789  ...      0.000000      0.000000   \n",
       "19997  0.067383  0.055664  0.103516  ...      0.000000      0.003906   \n",
       "19998  0.083984  0.045898  0.096680  ...      0.000000      0.000000   \n",
       "19999  0.069336  0.054688  0.101562  ...      0.000000      0.000000   \n",
       "\n",
       "       hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  hsv_h7_s3_v1  \\\n",
       "0          0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "1          0.018555      0.003906           0.0      0.000000      0.011719   \n",
       "2          0.000977      0.000000           0.0      0.000000      0.000000   \n",
       "3          0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "4          0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "19995      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "19996      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "19997      0.000000      0.000000           0.0      0.000977      0.000000   \n",
       "19998      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "19999      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "\n",
       "       hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0          0.000000           0.0      0  \n",
       "1          0.003906           0.0      0  \n",
       "2          0.000000           0.0      0  \n",
       "3          0.000000           0.0      0  \n",
       "4          0.000000           0.0      0  \n",
       "...             ...           ...    ...  \n",
       "19995      0.000000           0.0      1  \n",
       "19996      0.000000           0.0      1  \n",
       "19997      0.000000           0.0      1  \n",
       "19998      0.000000           0.0      1  \n",
       "19999      0.000000           0.0      1  \n",
       "\n",
       "[20000 rows x 527 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e399ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.027085</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.015226</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>0.015012</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.006252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016755</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.020906</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.007945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.015775</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>0.012243</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.005195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>0.007593</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.007213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.002148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.014144</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.012557</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.017888</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.011663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.023224</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.010277</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.010091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.024592</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.028831</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.019510</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.015365</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.011870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.018583</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.020778</td>\n",
       "      <td>0.030662</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.007055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0                0.019114           0.011690             0.013208   \n",
       "1                0.016755           0.014683             0.020438   \n",
       "2                0.021395           0.015775             0.009892   \n",
       "3                0.018256           0.014299             0.014059   \n",
       "4                0.004899           0.003762             0.004362   \n",
       "...                   ...                ...                  ...   \n",
       "19995            0.014299           0.014697             0.014144   \n",
       "19996            0.015771           0.015738             0.012362   \n",
       "19997            0.024592           0.025211             0.014526   \n",
       "19998            0.004713           0.004573             0.005187   \n",
       "19999            0.005310           0.006827             0.005772   \n",
       "\n",
       "       gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0                0.008110             0.027085            0.017980   \n",
       "1                0.015542             0.024632            0.020906   \n",
       "2                0.007718             0.008575            0.008443   \n",
       "3                0.010218             0.012424            0.008191   \n",
       "4                0.003657             0.006170            0.004497   \n",
       "...                   ...                  ...                 ...   \n",
       "19995            0.013913             0.033609            0.022614   \n",
       "19996            0.010676             0.023224            0.016746   \n",
       "19997            0.012060             0.028831            0.018488   \n",
       "19998            0.004802             0.016036            0.016051   \n",
       "19999            0.006579             0.016060            0.018583   \n",
       "\n",
       "       gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0                  0.014299             0.007634            0.015226   \n",
       "1                  0.014697             0.015717            0.010085   \n",
       "2                  0.010142             0.007819            0.012059   \n",
       "3                  0.012866             0.009325            0.012865   \n",
       "4                  0.004168             0.003116            0.004336   \n",
       "...                     ...                  ...                 ...   \n",
       "19995              0.013248             0.013862            0.010164   \n",
       "19996              0.018841             0.016119            0.009160   \n",
       "19997              0.015063             0.012784            0.014732   \n",
       "19998              0.005165             0.004952            0.003121   \n",
       "19999              0.006342             0.007145            0.007624   \n",
       "\n",
       "       gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0               0.013386             0.010334            0.007067   \n",
       "1               0.010674             0.010974            0.010191   \n",
       "2               0.012243             0.006099            0.005641   \n",
       "3               0.012881             0.008574            0.006384   \n",
       "4               0.003628             0.002837            0.002456   \n",
       "...                  ...                  ...                 ...   \n",
       "19995           0.012557             0.008433            0.010102   \n",
       "19996           0.011407             0.010277            0.010375   \n",
       "19997           0.019510             0.009216            0.009262   \n",
       "19998           0.005692             0.002702            0.002666   \n",
       "19999           0.014423             0.005144            0.006877   \n",
       "\n",
       "       gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0                 0.018516            0.015012              0.009861   \n",
       "1                 0.011706            0.012159              0.008319   \n",
       "2                 0.005278            0.004273              0.005680   \n",
       "3                 0.007717            0.007593              0.009149   \n",
       "4                 0.002710            0.002006              0.002680   \n",
       "...                    ...                 ...                   ...   \n",
       "19995             0.017888            0.013578              0.009319   \n",
       "19996             0.019426            0.017686              0.010390   \n",
       "19997             0.015365            0.012708              0.010608   \n",
       "19998             0.009823            0.011978              0.002969   \n",
       "19999             0.020778            0.030662              0.005206   \n",
       "\n",
       "       gabor_f0.4_t135_std  \n",
       "0                 0.006252  \n",
       "1                 0.007945  \n",
       "2                 0.005195  \n",
       "3                 0.007213  \n",
       "4                 0.002148  \n",
       "...                    ...  \n",
       "19995             0.011663  \n",
       "19996             0.010091  \n",
       "19997             0.011870  \n",
       "19998             0.003287  \n",
       "19999             0.007055  \n",
       "\n",
       "[20000 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_test[GABOR_ONLY_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d62f5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running experiments for feature set: LBP\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.01 seconds for 100000 samples\n",
      "Prediction time: 0.00 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6131    0.7958    0.6926     10000\n",
      "        Fake     0.7091    0.4978    0.5850     10000\n",
      "\n",
      "    accuracy                         0.6468     20000\n",
      "   macro avg     0.6611    0.6468    0.6388     20000\n",
      "weighted avg     0.6611    0.6468    0.6388     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.10 seconds for 100000 samples\n",
      "Prediction time: 1.21 seconds for 20000 samples\n",
      "Model size (joblib): 2.578 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6718    0.6904    0.6810     10000\n",
      "        Fake     0.6816    0.6627    0.6720     10000\n",
      "\n",
      "    accuracy                         0.6765     20000\n",
      "   macro avg     0.6767    0.6765    0.6765     20000\n",
      "weighted avg     0.6767    0.6765    0.6765     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 8.23 seconds for 100000 samples\n",
      "Prediction time: 0.00 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6749    0.7269    0.7000     10000\n",
      "        Fake     0.7041    0.6499    0.6759     10000\n",
      "\n",
      "    accuracy                         0.6884     20000\n",
      "   macro avg     0.6895    0.6884    0.6879     20000\n",
      "weighted avg     0.6895    0.6884    0.6879     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 1.38 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 25.193 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7055    0.7509    0.7275     10000\n",
      "        Fake     0.7338    0.6866    0.7094     10000\n",
      "\n",
      "    accuracy                         0.7188     20000\n",
      "   macro avg     0.7197    0.7188    0.7185     20000\n",
      "weighted avg     0.7197    0.7188    0.7185     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 0.15 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.130 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7066    0.7492    0.7273     10000\n",
      "        Fake     0.7331    0.6889    0.7103     10000\n",
      "\n",
      "    accuracy                         0.7190     20000\n",
      "   macro avg     0.7198    0.7190    0.7188     20000\n",
      "weighted avg     0.7198    0.7190    0.7188     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: HOG\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.17 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 0.006 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6470    0.6584    0.6527     10000\n",
      "        Fake     0.6523    0.6408    0.6465     10000\n",
      "\n",
      "    accuracy                         0.6496     20000\n",
      "   macro avg     0.6496    0.6496    0.6496     20000\n",
      "weighted avg     0.6496    0.6496    0.6496     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.05 seconds for 100000 samples\n",
      "Prediction time: 6.47 seconds for 20000 samples\n",
      "Model size (joblib): 108.273 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7333    0.7715    0.7519     10000\n",
      "        Fake     0.7589    0.7194    0.7386     10000\n",
      "\n",
      "    accuracy                         0.7454     20000\n",
      "   macro avg     0.7461    0.7454    0.7453     20000\n",
      "weighted avg     0.7461    0.7454    0.7453     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 5.85 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.003 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7137    0.7044    0.7090     10000\n",
      "        Fake     0.7082    0.7174    0.7128     10000\n",
      "\n",
      "    accuracy                         0.7109     20000\n",
      "   macro avg     0.7109    0.7109    0.7109     20000\n",
      "weighted avg     0.7109    0.7109    0.7109     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.59 seconds for 100000 samples\n",
      "Prediction time: 0.08 seconds for 20000 samples\n",
      "Model size (joblib): 20.033 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7351    0.7660    0.7502     10000\n",
      "        Fake     0.7557    0.7240    0.7395     10000\n",
      "\n",
      "    accuracy                         0.7450     20000\n",
      "   macro avg     0.7454    0.7450    0.7449     20000\n",
      "weighted avg     0.7454    0.7450    0.7449     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 2.60 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.152 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7613    0.7894    0.7751     10000\n",
      "        Fake     0.7813    0.7525    0.7666     10000\n",
      "\n",
      "    accuracy                         0.7710     20000\n",
      "   macro avg     0.7713    0.7710    0.7709     20000\n",
      "weighted avg     0.7713    0.7710    0.7709     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: Gabor\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.01 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6685    0.8013    0.7289     10000\n",
      "        Fake     0.7520    0.6026    0.6691     10000\n",
      "\n",
      "    accuracy                         0.7019     20000\n",
      "   macro avg     0.7103    0.7020    0.6990     20000\n",
      "weighted avg     0.7103    0.7019    0.6990     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.00 seconds for 100000 samples\n",
      "Prediction time: 0.76 seconds for 20000 samples\n",
      "Model size (joblib): 5.281 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8308    0.8282    0.8295     10000\n",
      "        Fake     0.8287    0.8313    0.8300     10000\n",
      "\n",
      "    accuracy                         0.8297     20000\n",
      "   macro avg     0.8298    0.8297    0.8297     20000\n",
      "weighted avg     0.8298    0.8297    0.8297     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 3.53 seconds for 100000 samples\n",
      "Prediction time: 0.00 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7129    0.7792    0.7446     10000\n",
      "        Fake     0.7566    0.6862    0.7197     10000\n",
      "\n",
      "    accuracy                         0.7327     20000\n",
      "   macro avg     0.7347    0.7327    0.7321     20000\n",
      "weighted avg     0.7347    0.7327    0.7321     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 3.80 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 16.931 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8682    0.8513    0.8597     10000\n",
      "        Fake     0.8541    0.8708    0.8624     10000\n",
      "\n",
      "    accuracy                         0.8610     20000\n",
      "   macro avg     0.8612    0.8610    0.8610     20000\n",
      "weighted avg     0.8612    0.8610    0.8610     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 0.19 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.141 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8686    0.8461    0.8572     10000\n",
      "        Fake     0.8500    0.8720    0.8609     10000\n",
      "\n",
      "    accuracy                         0.8590     20000\n",
      "   macro avg     0.8593    0.8590    0.8590     20000\n",
      "weighted avg     0.8593    0.8590    0.8590     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: Color\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.09 seconds for 100000 samples\n",
      "Prediction time: 0.04 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7589    0.4818    0.5894     10000\n",
      "        Fake     0.6204    0.8469    0.7162     10000\n",
      "\n",
      "    accuracy                         0.6643     20000\n",
      "   macro avg     0.6896    0.6643    0.6528     20000\n",
      "weighted avg     0.6896    0.6643    0.6528     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.02 seconds for 100000 samples\n",
      "Prediction time: 2.45 seconds for 20000 samples\n",
      "Model size (joblib): 11.931 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7671    0.7735    0.7703     10000\n",
      "        Fake     0.7716    0.7652    0.7684     10000\n",
      "\n",
      "    accuracy                         0.7693     20000\n",
      "   macro avg     0.7694    0.7693    0.7693     20000\n",
      "weighted avg     0.7694    0.7693    0.7693     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 3.71 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.002 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7299    0.7968    0.7619     10000\n",
      "        Fake     0.7763    0.7052    0.7390     10000\n",
      "\n",
      "    accuracy                         0.7510     20000\n",
      "   macro avg     0.7531    0.7510    0.7505     20000\n",
      "weighted avg     0.7531    0.7510    0.7505     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 4.25 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 17.944 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8290    0.8482    0.8385     10000\n",
      "        Fake     0.8446    0.8251    0.8347     10000\n",
      "\n",
      "    accuracy                         0.8367     20000\n",
      "   macro avg     0.8368    0.8366    0.8366     20000\n",
      "weighted avg     0.8368    0.8367    0.8366     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 0.99 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.135 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8320    0.8423    0.8371     10000\n",
      "        Fake     0.8403    0.8299    0.8351     10000\n",
      "\n",
      "    accuracy                         0.8361     20000\n",
      "   macro avg     0.8362    0.8361    0.8361     20000\n",
      "weighted avg     0.8362    0.8361    0.8361     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.31 seconds for 100000 samples\n",
      "Prediction time: 0.13 seconds for 20000 samples\n",
      "Model size (joblib): 0.009 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8160    0.6886    0.7469     10000\n",
      "        Fake     0.7306    0.8447    0.7835     10000\n",
      "\n",
      "    accuracy                         0.7667     20000\n",
      "   macro avg     0.7733    0.7667    0.7652     20000\n",
      "weighted avg     0.7733    0.7667    0.7652     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 6.04 seconds for 20000 samples\n",
      "Model size (joblib): 131.268 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7562    0.7815    0.7687     10000\n",
      "        Fake     0.7739    0.7481    0.7608     10000\n",
      "\n",
      "    accuracy                         0.7648     20000\n",
      "   macro avg     0.7651    0.7648    0.7647     20000\n",
      "weighted avg     0.7651    0.7648    0.7647     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 10.05 seconds for 100000 samples\n",
      "Prediction time: 0.04 seconds for 20000 samples\n",
      "Model size (joblib): 0.005 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8563    0.8762    0.8662     10000\n",
      "        Fake     0.8733    0.8530    0.8630     10000\n",
      "\n",
      "    accuracy                         0.8646     20000\n",
      "   macro avg     0.8648    0.8646    0.8646     20000\n",
      "weighted avg     0.8648    0.8646    0.8646     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.37 seconds for 100000 samples\n",
      "Prediction time: 0.08 seconds for 20000 samples\n",
      "Model size (joblib): 13.777 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9124    0.9169    0.9147     10000\n",
      "        Fake     0.9165    0.9120    0.9142     10000\n",
      "\n",
      "    accuracy                         0.9144     20000\n",
      "   macro avg     0.9145    0.9144    0.9144     20000\n",
      "weighted avg     0.9145    0.9144    0.9144     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 4.19 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.147 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9342    0.9335    0.9338     10000\n",
      "        Fake     0.9335    0.9342    0.9339     10000\n",
      "\n",
      "    accuracy                         0.9338     20000\n",
      "   macro avg     0.9339    0.9339    0.9338     20000\n",
      "weighted avg     0.9339    0.9338    0.9338     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature_set_name, feature_cols in FEATURE_SETS.items():\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Running experiments for feature set: {feature_set_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    X_train = df_all_train[feature_cols]\n",
    "    X_test = df_all_test[feature_cols]\n",
    "    y_train = df_all_train[\"label\"]\n",
    "    y_test = df_all_test[\"label\"]\n",
    "    X_train, y_train = shuffle_indexes(X_train, y_train)\n",
    "    run_ml_experiments(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        path_output=Path(\"outputs/\"),\n",
    "        feature_set_name=feature_set_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2feaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = joblib.load(\"outputs/models/All/LGBMClassifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22189b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9342    0.9335    0.9338     10000\n",
      "        Fake     0.9335    0.9342    0.9339     10000\n",
      "\n",
      "    accuracy                         0.9338     20000\n",
      "   macro avg     0.9339    0.9339    0.9338     20000\n",
      "weighted avg     0.9339    0.9338    0.9338     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, best_model.predict(X_test), target_names=[\"Real\", \"Fake\"], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc70c457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "feature_cols = df_all_train.columns.drop(\"label\").tolist()\n",
    "# Get indices of top 20 importances\n",
    "top_idx = np.argsort(importances)[-20:]\n",
    "top_features = [feature_cols[i] for i in top_idx]\n",
    "top_importances = importances[top_idx]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(top_features, top_importances)\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Top 20 Feature Importances of LightGBM Model with All Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "448ccbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SETS_ABLATION = {\n",
    "    \"Gabor+Color\": GABOR_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noLBP\": HOG_ONLY_COLS + GABOR_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noHOG\": LBP_ONLY_COLS + GABOR_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noGabor\": LBP_ONLY_COLS + HOG_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noColor\": LBP_ONLY_COLS + HOG_ONLY_COLS + GABOR_ONLY_COLS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff44f8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running experiments for feature set: Gabor+Color\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7914    0.5090    0.6195     10000\n",
      "        Fake     0.6381    0.8658    0.7347     10000\n",
      "\n",
      "    accuracy                         0.6874     20000\n",
      "   macro avg     0.7147    0.6874    0.6771     20000\n",
      "weighted avg     0.7147    0.6874    0.6771     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.04 seconds for 100000 samples\n",
      "Prediction time: 2.56 seconds for 20000 samples\n",
      "Model size (joblib): 18.190 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7697    0.7871    0.7783     10000\n",
      "        Fake     0.7822    0.7645    0.7732     10000\n",
      "\n",
      "    accuracy                         0.7758     20000\n",
      "   macro avg     0.7759    0.7758    0.7758     20000\n",
      "weighted avg     0.7759    0.7758    0.7758     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 4.63 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.002 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7916    0.8393    0.8147     10000\n",
      "        Fake     0.8290    0.7790    0.8032     10000\n",
      "\n",
      "    accuracy                         0.8092     20000\n",
      "   macro avg     0.8103    0.8092    0.8090     20000\n",
      "weighted avg     0.8103    0.8092    0.8090     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 4.80 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 14.112 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9185    0.9191    0.9188     10000\n",
      "        Fake     0.9191    0.9185    0.9188     10000\n",
      "\n",
      "    accuracy                         0.9188     20000\n",
      "   macro avg     0.9188    0.9188    0.9188     20000\n",
      "weighted avg     0.9188    0.9188    0.9188     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 1.26 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.141 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9290    0.9225    0.9257     10000\n",
      "        Fake     0.9230    0.9295    0.9263     10000\n",
      "\n",
      "    accuracy                         0.9260     20000\n",
      "   macro avg     0.9260    0.9260    0.9260     20000\n",
      "weighted avg     0.9260    0.9260    0.9260     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noLBP\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.34 seconds for 100000 samples\n",
      "Prediction time: 0.13 seconds for 20000 samples\n",
      "Model size (joblib): 0.009 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8106    0.6763    0.7374     10000\n",
      "        Fake     0.7223    0.8420    0.7776     10000\n",
      "\n",
      "    accuracy                         0.7591     20000\n",
      "   macro avg     0.7665    0.7591    0.7575     20000\n",
      "weighted avg     0.7665    0.7591    0.7575     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 6.10 seconds for 20000 samples\n",
      "Model size (joblib): 129.623 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7550    0.7795    0.7670     10000\n",
      "        Fake     0.7721    0.7470    0.7593     10000\n",
      "\n",
      "    accuracy                         0.7632     20000\n",
      "   macro avg     0.7635    0.7632    0.7632     20000\n",
      "weighted avg     0.7635    0.7632    0.7632     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 5.99 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.005 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8383    0.8541    0.8461     10000\n",
      "        Fake     0.8513    0.8353    0.8432     10000\n",
      "\n",
      "    accuracy                         0.8447     20000\n",
      "   macro avg     0.8448    0.8447    0.8447     20000\n",
      "weighted avg     0.8448    0.8447    0.8447     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.65 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 13.960 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9058    0.9149    0.9103     10000\n",
      "        Fake     0.9140    0.9049    0.9094     10000\n",
      "\n",
      "    accuracy                         0.9099     20000\n",
      "   macro avg     0.9099    0.9099    0.9099     20000\n",
      "weighted avg     0.9099    0.9099    0.9099     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 4.31 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.148 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9300    0.9276    0.9288     10000\n",
      "        Fake     0.9278    0.9302    0.9290     10000\n",
      "\n",
      "    accuracy                         0.9289     20000\n",
      "   macro avg     0.9289    0.9289    0.9289     20000\n",
      "weighted avg     0.9289    0.9289    0.9289     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noHOG\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8035    0.5254    0.6353     10000\n",
      "        Fake     0.6474    0.8715    0.7429     10000\n",
      "\n",
      "    accuracy                         0.6985     20000\n",
      "   macro avg     0.7255    0.6985    0.6891     20000\n",
      "weighted avg     0.7255    0.6985    0.6891     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.04 seconds for 100000 samples\n",
      "Prediction time: 2.73 seconds for 20000 samples\n",
      "Model size (joblib): 19.680 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8002    0.8283    0.8140     10000\n",
      "        Fake     0.8221    0.7932    0.8074     10000\n",
      "\n",
      "    accuracy                         0.8107     20000\n",
      "   macro avg     0.8111    0.8108    0.8107     20000\n",
      "weighted avg     0.8111    0.8107    0.8107     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 4.57 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.003 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8316    0.8679    0.8494     10000\n",
      "        Fake     0.8619    0.8243    0.8427     10000\n",
      "\n",
      "    accuracy                         0.8461     20000\n",
      "   macro avg     0.8468    0.8461    0.8460     20000\n",
      "weighted avg     0.8468    0.8461    0.8460     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 5.30 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 13.267 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9256    0.9293    0.9274     10000\n",
      "        Fake     0.9290    0.9253    0.9272     10000\n",
      "\n",
      "    accuracy                         0.9273     20000\n",
      "   macro avg     0.9273    0.9273    0.9273     20000\n",
      "weighted avg     0.9273    0.9273    0.9273     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 1.31 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.140 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9324    0.9300    0.9312     10000\n",
      "        Fake     0.9302    0.9326    0.9314     10000\n",
      "\n",
      "    accuracy                         0.9313     20000\n",
      "   macro avg     0.9313    0.9313    0.9313     20000\n",
      "weighted avg     0.9313    0.9313    0.9313     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noGabor\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.30 seconds for 100000 samples\n",
      "Prediction time: 0.12 seconds for 20000 samples\n",
      "Model size (joblib): 0.009 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7942    0.6666    0.7248     10000\n",
      "        Fake     0.7128    0.8273    0.7658     10000\n",
      "\n",
      "    accuracy                         0.7470     20000\n",
      "   macro avg     0.7535    0.7470    0.7453     20000\n",
      "weighted avg     0.7535    0.7470    0.7453     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 6.08 seconds for 20000 samples\n",
      "Model size (joblib): 125.477 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7561    0.7808    0.7683     10000\n",
      "        Fake     0.7734    0.7482    0.7606     10000\n",
      "\n",
      "    accuracy                         0.7645     20000\n",
      "   macro avg     0.7648    0.7645    0.7644     20000\n",
      "weighted avg     0.7648    0.7645    0.7644     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 8.21 seconds for 100000 samples\n",
      "Prediction time: 0.04 seconds for 20000 samples\n",
      "Model size (joblib): 0.005 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8295    0.8552    0.8421     10000\n",
      "        Fake     0.8506    0.8242    0.8372     10000\n",
      "\n",
      "    accuracy                         0.8397     20000\n",
      "   macro avg     0.8400    0.8397    0.8397     20000\n",
      "weighted avg     0.8400    0.8397    0.8397     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.49 seconds for 100000 samples\n",
      "Prediction time: 0.09 seconds for 20000 samples\n",
      "Model size (joblib): 15.324 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8709    0.8888    0.8797     10000\n",
      "        Fake     0.8865    0.8682    0.8772     10000\n",
      "\n",
      "    accuracy                         0.8785     20000\n",
      "   macro avg     0.8787    0.8785    0.8785     20000\n",
      "weighted avg     0.8787    0.8785    0.8785     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 4.42 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.146 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8871    0.8957    0.8914     10000\n",
      "        Fake     0.8947    0.8860    0.8903     10000\n",
      "\n",
      "    accuracy                         0.8909     20000\n",
      "   macro avg     0.8909    0.8909    0.8908     20000\n",
      "weighted avg     0.8909    0.8909    0.8908     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noColor\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.21 seconds for 100000 samples\n",
      "Prediction time: 0.09 seconds for 20000 samples\n",
      "Model size (joblib): 0.006 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6797    0.6983    0.6889     10000\n",
      "        Fake     0.6898    0.6710    0.6803     10000\n",
      "\n",
      "    accuracy                         0.6846     20000\n",
      "   macro avg     0.6848    0.6846    0.6846     20000\n",
      "weighted avg     0.6848    0.6846    0.6846     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.08 seconds for 100000 samples\n",
      "Prediction time: 5.14 seconds for 20000 samples\n",
      "Model size (joblib): 115.929 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7357    0.7740    0.7543     10000\n",
      "        Fake     0.7616    0.7219    0.7412     10000\n",
      "\n",
      "    accuracy                         0.7480     20000\n",
      "   macro avg     0.7486    0.7480    0.7478     20000\n",
      "weighted avg     0.7486    0.7480    0.7478     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 9.06 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7957    0.8068    0.8012     10000\n",
      "        Fake     0.8041    0.7929    0.7984     10000\n",
      "\n",
      "    accuracy                         0.7998     20000\n",
      "   macro avg     0.7999    0.7998    0.7998     20000\n",
      "weighted avg     0.7999    0.7998    0.7998     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 20.56 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 16.247 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8542    0.8619    0.8580     10000\n",
      "        Fake     0.8606    0.8529    0.8568     10000\n",
      "\n",
      "    accuracy                         0.8574     20000\n",
      "   macro avg     0.8574    0.8574    0.8574     20000\n",
      "weighted avg     0.8574    0.8574    0.8574     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 2.98 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.149 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8935    0.8905    0.8920     10000\n",
      "        Fake     0.8909    0.8939    0.8924     10000\n",
      "\n",
      "    accuracy                         0.8922     20000\n",
      "   macro avg     0.8922    0.8922    0.8922     20000\n",
      "weighted avg     0.8922    0.8922    0.8922     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature_set_name, feature_cols in FEATURE_SETS_ABLATION.items():\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Running experiments for feature set: {feature_set_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    X_train = df_all_train[feature_cols]\n",
    "    X_test = df_all_test[feature_cols]\n",
    "    y_train = df_all_train[\"label\"]\n",
    "    y_test = df_all_test[\"label\"]\n",
    "    X_train, y_train = shuffle_indexes(X_train, y_train)\n",
    "    run_ml_experiments(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        path_output=Path(\"outputs/\"),\n",
    "        feature_set_name=feature_set_name,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "at82-08-ai-generated-image-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
