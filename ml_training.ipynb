{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b354fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.ml_training import run_ml_experiments, shuffle_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b2da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbp_train = pd.read_csv(\"data/lbp_features_train.csv\", dtype=\"float32\")\n",
    "df_lbp_test = pd.read_csv(\"data/lbp_features_test.csv\", dtype=\"float32\")\n",
    "df_lbp_train[\"label\"] = df_lbp_train[\"label\"].astype(\"uint8\")\n",
    "df_lbp_test[\"label\"] = df_lbp_test[\"label\"].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb344049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0.276367</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0      0.070312  0.095703  0.058594  0.111328  0.200195  0.151367  0.059570   \n",
       "1      0.062500  0.091797  0.062500  0.105469  0.244141  0.124023  0.046875   \n",
       "2      0.068359  0.095703  0.064453  0.126953  0.177734  0.144531  0.076172   \n",
       "3      0.048828  0.084961  0.065430  0.096680  0.247070  0.145508  0.061523   \n",
       "4      0.065430  0.103516  0.081055  0.110352  0.171875  0.118164  0.093750   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  0.053711  0.096680  0.051758  0.131836  0.244141  0.147461  0.052734   \n",
       "99996  0.041016  0.056641  0.041992  0.122070  0.276367  0.180664  0.063477   \n",
       "99997  0.068359  0.113281  0.055664  0.085938  0.205078  0.113281  0.045898   \n",
       "99998  0.044922  0.119141  0.058594  0.124023  0.193359  0.126953  0.064453   \n",
       "99999  0.043945  0.077148  0.040039  0.096680  0.174805  0.214844  0.079102   \n",
       "\n",
       "          LBP_7     LBP_8     LBP_9  label  \n",
       "0      0.070312  0.050781  0.131836      0  \n",
       "1      0.073242  0.059570  0.129883      0  \n",
       "2      0.066406  0.054688  0.125000      0  \n",
       "3      0.075195  0.069336  0.105469      0  \n",
       "4      0.064453  0.060547  0.130859      0  \n",
       "...         ...       ...       ...    ...  \n",
       "99995  0.069336  0.053711  0.098633      1  \n",
       "99996  0.054688  0.068359  0.094727      1  \n",
       "99997  0.086914  0.088867  0.136719      1  \n",
       "99998  0.093750  0.050781  0.124023      1  \n",
       "99999  0.091797  0.083984  0.097656      1  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b299fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.092773</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0      0.074219  0.099609  0.061523  0.117188  0.161133  0.127930  0.076172   \n",
       "1      0.075195  0.092773  0.057617  0.117188  0.190430  0.128906  0.072266   \n",
       "2      0.067383  0.082031  0.067383  0.130859  0.201172  0.138672  0.073242   \n",
       "3      0.063477  0.105469  0.073242  0.111328  0.190430  0.112305  0.050781   \n",
       "4      0.039062  0.086914  0.069336  0.161133  0.194336  0.159180  0.078125   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995  0.069336  0.099609  0.061523  0.100586  0.199219  0.133789  0.052734   \n",
       "19996  0.078125  0.111328  0.055664  0.109375  0.181641  0.109375  0.060547   \n",
       "19997  0.062500  0.076172  0.083984  0.127930  0.237305  0.129883  0.055664   \n",
       "19998  0.043945  0.094727  0.071289  0.106445  0.246094  0.154297  0.056641   \n",
       "19999  0.043945  0.100586  0.039062  0.094727  0.246094  0.213867  0.036133   \n",
       "\n",
       "          LBP_7     LBP_8     LBP_9  label  \n",
       "0      0.076172  0.058594  0.147461      0  \n",
       "1      0.062500  0.067383  0.135742      0  \n",
       "2      0.068359  0.050781  0.120117      0  \n",
       "3      0.090820  0.069336  0.132812      0  \n",
       "4      0.071289  0.049805  0.090820      0  \n",
       "...         ...       ...       ...    ...  \n",
       "19995  0.083008  0.061523  0.138672      1  \n",
       "19996  0.085938  0.074219  0.133789      1  \n",
       "19997  0.067383  0.055664  0.103516      1  \n",
       "19998  0.083984  0.045898  0.096680      1  \n",
       "19999  0.069336  0.054688  0.101562      1  \n",
       "\n",
       "[20000 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab9c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   LBP_0   100000 non-null  float32\n",
      " 1   LBP_1   100000 non-null  float32\n",
      " 2   LBP_2   100000 non-null  float32\n",
      " 3   LBP_3   100000 non-null  float32\n",
      " 4   LBP_4   100000 non-null  float32\n",
      " 5   LBP_5   100000 non-null  float32\n",
      " 6   LBP_6   100000 non-null  float32\n",
      " 7   LBP_7   100000 non-null  float32\n",
      " 8   LBP_8   100000 non-null  float32\n",
      " 9   LBP_9   100000 non-null  float32\n",
      " 10  label   100000 non-null  uint8  \n",
      "dtypes: float32(10), uint8(1)\n",
      "memory usage: 3.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_lbp_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1838a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   LBP_0   20000 non-null  float32\n",
      " 1   LBP_1   20000 non-null  float32\n",
      " 2   LBP_2   20000 non-null  float32\n",
      " 3   LBP_3   20000 non-null  float32\n",
      " 4   LBP_4   20000 non-null  float32\n",
      " 5   LBP_5   20000 non-null  float32\n",
      " 6   LBP_6   20000 non-null  float32\n",
      " 7   LBP_7   20000 non-null  float32\n",
      " 8   LBP_8   20000 non-null  float32\n",
      " 9   LBP_9   20000 non-null  float32\n",
      " 10  label   20000 non-null  uint8  \n",
      "dtypes: float32(10), uint8(1)\n",
      "memory usage: 800.9 KB\n"
     ]
    }
   ],
   "source": [
    "df_lbp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf48fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_lbp_train.drop(\"label\", axis=1)\n",
    "y_train = df_lbp_train[\"label\"]\n",
    "X_test = df_lbp_test.drop(\"label\", axis=1)\n",
    "y_test = df_lbp_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e87c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle_indexes(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b97b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GaussianNB:\n",
      "Training time (sec): 0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6131    0.7958    0.6926     10000\n",
      "        Fake     0.7091    0.4978    0.5850     10000\n",
      "\n",
      "    accuracy                         0.6468     20000\n",
      "   macro avg     0.6611    0.6468    0.6388     20000\n",
      "weighted avg     0.6611    0.6468    0.6388     20000\n",
      "\n",
      "Model size (joblib): 0.001 MB\n",
      "\n",
      "KNN:\n",
      "Training time (sec): 0.11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6718    0.6904    0.6810     10000\n",
      "        Fake     0.6816    0.6627    0.6720     10000\n",
      "\n",
      "    accuracy                         0.6765     20000\n",
      "   macro avg     0.6767    0.6765    0.6765     20000\n",
      "weighted avg     0.6767    0.6765    0.6765     20000\n",
      "\n",
      "Model size (joblib): 2.578 MB\n",
      "\n",
      "LogisticRegression:\n",
      "Training time (sec): 6.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6749    0.7269    0.7000     10000\n",
      "        Fake     0.7041    0.6499    0.6759     10000\n",
      "\n",
      "    accuracy                         0.6884     20000\n",
      "   macro avg     0.6895    0.6884    0.6879     20000\n",
      "weighted avg     0.6895    0.6884    0.6879     20000\n",
      "\n",
      "Model size (joblib): 0.001 MB\n",
      "\n",
      "RandomForest:\n",
      "Training time (sec): 1.41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7055    0.7509    0.7275     10000\n",
      "        Fake     0.7338    0.6866    0.7094     10000\n",
      "\n",
      "    accuracy                         0.7188     20000\n",
      "   macro avg     0.7197    0.7188    0.7185     20000\n",
      "weighted avg     0.7197    0.7188    0.7185     20000\n",
      "\n",
      "Model size (joblib): 25.193 MB\n",
      "\n",
      "LGBMClassifier:\n",
      "Training time (sec): 0.17\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7066    0.7492    0.7273     10000\n",
      "        Fake     0.7331    0.6889    0.7103     10000\n",
      "\n",
      "    accuracy                         0.7190     20000\n",
      "   macro avg     0.7198    0.7190    0.7188     20000\n",
      "weighted avg     0.7198    0.7190    0.7188     20000\n",
      "\n",
      "Model size (joblib): 0.130 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'GaussianNB': {'training_time': 0.009917020797729492,\n",
       "  'report': {'Real': {'precision': 0.6130970724191063,\n",
       "    'recall': 0.7958,\n",
       "    'f1-score': 0.6926022628372498,\n",
       "    'support': 10000.0},\n",
       "   'Fake': {'precision': 0.7091168091168091,\n",
       "    'recall': 0.4978,\n",
       "    'f1-score': 0.5849588719153936,\n",
       "    'support': 10000.0},\n",
       "   'accuracy': 0.6468,\n",
       "   'macro avg': {'precision': 0.6611069407679577,\n",
       "    'recall': 0.6468,\n",
       "    'f1-score': 0.6387805673763217,\n",
       "    'support': 20000.0},\n",
       "   'weighted avg': {'precision': 0.6611069407679577,\n",
       "    'recall': 0.6468,\n",
       "    'f1-score': 0.6387805673763218,\n",
       "    'support': 20000.0}},\n",
       "  'report_str': '              precision    recall  f1-score   support\\n\\n        Real     0.6131    0.7958    0.6926     10000\\n        Fake     0.7091    0.4978    0.5850     10000\\n\\n    accuracy                         0.6468     20000\\n   macro avg     0.6611    0.6468    0.6388     20000\\nweighted avg     0.6611    0.6468    0.6388     20000\\n',\n",
       "  'model_size_mb': 0.0007953643798828125},\n",
       " 'KNN': {'training_time': 0.11184811592102051,\n",
       "  'report': {'Real': {'precision': 0.6717913788070449,\n",
       "    'recall': 0.6904,\n",
       "    'f1-score': 0.6809685850964147,\n",
       "    'support': 10000.0},\n",
       "   'Fake': {'precision': 0.681579759333539,\n",
       "    'recall': 0.6627,\n",
       "    'f1-score': 0.6720073011205192,\n",
       "    'support': 10000.0},\n",
       "   'accuracy': 0.67655,\n",
       "   'macro avg': {'precision': 0.676685569070292,\n",
       "    'recall': 0.67655,\n",
       "    'f1-score': 0.6764879431084669,\n",
       "    'support': 20000.0},\n",
       "   'weighted avg': {'precision': 0.676685569070292,\n",
       "    'recall': 0.67655,\n",
       "    'f1-score': 0.6764879431084669,\n",
       "    'support': 20000.0}},\n",
       "  'report_str': '              precision    recall  f1-score   support\\n\\n        Real     0.6718    0.6904    0.6810     10000\\n        Fake     0.6816    0.6627    0.6720     10000\\n\\n    accuracy                         0.6765     20000\\n   macro avg     0.6767    0.6765    0.6765     20000\\nweighted avg     0.6767    0.6765    0.6765     20000\\n',\n",
       "  'model_size_mb': 2.5779647827148438},\n",
       " 'LogisticRegression': {'training_time': 6.827889442443848,\n",
       "  'report': {'Real': {'precision': 0.6749303621169916,\n",
       "    'recall': 0.7269,\n",
       "    'f1-score': 0.6999518536350505,\n",
       "    'support': 10000.0},\n",
       "   'Fake': {'precision': 0.7041170097508126,\n",
       "    'recall': 0.6499,\n",
       "    'f1-score': 0.6759230369214768,\n",
       "    'support': 10000.0},\n",
       "   'accuracy': 0.6884,\n",
       "   'macro avg': {'precision': 0.6895236859339021,\n",
       "    'recall': 0.6884,\n",
       "    'f1-score': 0.6879374452782636,\n",
       "    'support': 20000.0},\n",
       "   'weighted avg': {'precision': 0.6895236859339021,\n",
       "    'recall': 0.6884,\n",
       "    'f1-score': 0.6879374452782636,\n",
       "    'support': 20000.0}},\n",
       "  'report_str': '              precision    recall  f1-score   support\\n\\n        Real     0.6749    0.7269    0.7000     10000\\n        Fake     0.7041    0.6499    0.6759     10000\\n\\n    accuracy                         0.6884     20000\\n   macro avg     0.6895    0.6884    0.6879     20000\\nweighted avg     0.6895    0.6884    0.6879     20000\\n',\n",
       "  'model_size_mb': 0.0007457733154296875},\n",
       " 'RandomForest': {'training_time': 1.4145362377166748,\n",
       "  'report': {'Real': {'precision': 0.7055341539039744,\n",
       "    'recall': 0.7509,\n",
       "    'f1-score': 0.7275105362592647,\n",
       "    'support': 10000.0},\n",
       "   'Fake': {'precision': 0.7337821951480176,\n",
       "    'recall': 0.6866,\n",
       "    'f1-score': 0.7094074495014724,\n",
       "    'support': 10000.0},\n",
       "   'accuracy': 0.71875,\n",
       "   'macro avg': {'precision': 0.719658174525996,\n",
       "    'recall': 0.71875,\n",
       "    'f1-score': 0.7184589928803685,\n",
       "    'support': 20000.0},\n",
       "   'weighted avg': {'precision': 0.719658174525996,\n",
       "    'recall': 0.71875,\n",
       "    'f1-score': 0.7184589928803685,\n",
       "    'support': 20000.0}},\n",
       "  'report_str': '              precision    recall  f1-score   support\\n\\n        Real     0.7055    0.7509    0.7275     10000\\n        Fake     0.7338    0.6866    0.7094     10000\\n\\n    accuracy                         0.7188     20000\\n   macro avg     0.7197    0.7188    0.7185     20000\\nweighted avg     0.7197    0.7188    0.7185     20000\\n',\n",
       "  'model_size_mb': 25.193442344665527},\n",
       " 'LGBMClassifier': {'training_time': 0.16750001907348633,\n",
       "  'report': {'Real': {'precision': 0.7065924738281618,\n",
       "    'recall': 0.7492,\n",
       "    'f1-score': 0.7272727272727273,\n",
       "    'support': 10000.0},\n",
       "   'Fake': {'precision': 0.7331063105246355,\n",
       "    'recall': 0.6889,\n",
       "    'f1-score': 0.7103160282517915,\n",
       "    'support': 10000.0},\n",
       "   'accuracy': 0.71905,\n",
       "   'macro avg': {'precision': 0.7198493921763987,\n",
       "    'recall': 0.71905,\n",
       "    'f1-score': 0.7187943777622594,\n",
       "    'support': 20000.0},\n",
       "   'weighted avg': {'precision': 0.7198493921763987,\n",
       "    'recall': 0.71905,\n",
       "    'f1-score': 0.7187943777622594,\n",
       "    'support': 20000.0}},\n",
       "  'report_str': '              precision    recall  f1-score   support\\n\\n        Real     0.7066    0.7492    0.7273     10000\\n        Fake     0.7331    0.6889    0.7103     10000\\n\\n    accuracy                         0.7190     20000\\n   macro avg     0.7198    0.7190    0.7188     20000\\nweighted avg     0.7198    0.7190    0.7188     20000\\n',\n",
       "  'model_size_mb': 0.12987518310546875}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_output = Path(\"outputs/lbp_classification_results.json\")\n",
    "results = run_ml_experiments(X_train, y_train, X_test, y_test, path_output)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "at82-08-ai-generated-image-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
