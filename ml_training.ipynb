{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils.ml_training import run_ml_experiments, shuffle_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b2da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbp_train = pd.read_csv(\"data/lbp_features_train.csv\", dtype=\"float32\")\n",
    "df_lbp_test = pd.read_csv(\"data/lbp_features_test.csv\", dtype=\"float32\")\n",
    "df_lbp_train[\"label\"] = df_lbp_train[\"label\"].astype(\"uint8\")\n",
    "df_lbp_test[\"label\"] = df_lbp_test[\"label\"].astype(\"uint8\")\n",
    "df_hog_train = pd.read_csv(\"data/hog_features_train.csv\", dtype=\"float32\")\n",
    "df_hog_test = pd.read_csv(\"data/hog_features_test.csv\", dtype=\"float32\")\n",
    "df_hog_train[\"label\"] = df_hog_train[\"label\"].astype(\"uint8\")\n",
    "df_hog_test[\"label\"] = df_hog_test[\"label\"].astype(\"uint8\")\n",
    "df_gabor_train = pd.read_csv(\"data/gabor_features_train.csv\", dtype=\"float32\")\n",
    "df_gabor_test = pd.read_csv(\"data/gabor_features_test.csv\", dtype=\"float32\")\n",
    "df_gabor_train[\"label\"] = df_gabor_train[\"label\"].astype(\"uint8\")\n",
    "df_gabor_test[\"label\"] = df_gabor_test[\"label\"].astype(\"uint8\")\n",
    "df_color_train = pd.read_csv(\"data/color_features_train.csv\", dtype=\"float32\")\n",
    "df_color_test = pd.read_csv(\"data/color_features_test.csv\", dtype=\"float32\")\n",
    "df_color_train[\"label\"] = df_color_train[\"label\"].astype(\"uint8\")\n",
    "df_color_test[\"label\"] = df_color_test[\"label\"].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb344049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_lbp_train[\"label\"].equals(df_hog_train[\"label\"])\n",
    "    and df_lbp_train[\"label\"].equals(df_gabor_train[\"label\"])\n",
    "    and df_lbp_train[\"label\"].equals(df_color_train[\"label\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b299fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0  0.070312  0.095703  0.058594  0.111328  0.200195  0.151367  0.059570   \n",
       "1  0.062500  0.091797  0.062500  0.105469  0.244141  0.124023  0.046875   \n",
       "2  0.068359  0.095703  0.064453  0.126953  0.177734  0.144531  0.076172   \n",
       "3  0.048828  0.084961  0.065430  0.096680  0.247070  0.145508  0.061523   \n",
       "4  0.065430  0.103516  0.081055  0.110352  0.171875  0.118164  0.093750   \n",
       "\n",
       "      LBP_7     LBP_8     LBP_9  label  \n",
       "0  0.070312  0.050781  0.131836      0  \n",
       "1  0.073242  0.059570  0.129883      0  \n",
       "2  0.066406  0.054688  0.125000      0  \n",
       "3  0.075195  0.069336  0.105469      0  \n",
       "4  0.064453  0.060547  0.130859      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df7b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.092773</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0  0.074219  0.099609  0.061523  0.117188  0.161133  0.127930  0.076172   \n",
       "1  0.075195  0.092773  0.057617  0.117188  0.190430  0.128906  0.072266   \n",
       "2  0.067383  0.082031  0.067383  0.130859  0.201172  0.138672  0.073242   \n",
       "3  0.063477  0.105469  0.073242  0.111328  0.190430  0.112305  0.050781   \n",
       "4  0.039062  0.086914  0.069336  0.161133  0.194336  0.159180  0.078125   \n",
       "\n",
       "      LBP_7     LBP_8     LBP_9  label  \n",
       "0  0.076172  0.058594  0.147461      0  \n",
       "1  0.062500  0.067383  0.135742      0  \n",
       "2  0.068359  0.050781  0.120117      0  \n",
       "3  0.090820  0.069336  0.132812      0  \n",
       "4  0.071289  0.049805  0.090820      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lbp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d746c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOG_0</th>\n",
       "      <th>HOG_1</th>\n",
       "      <th>HOG_2</th>\n",
       "      <th>HOG_3</th>\n",
       "      <th>HOG_4</th>\n",
       "      <th>HOG_5</th>\n",
       "      <th>HOG_6</th>\n",
       "      <th>HOG_7</th>\n",
       "      <th>HOG_8</th>\n",
       "      <th>HOG_9</th>\n",
       "      <th>...</th>\n",
       "      <th>HOG_315</th>\n",
       "      <th>HOG_316</th>\n",
       "      <th>HOG_317</th>\n",
       "      <th>HOG_318</th>\n",
       "      <th>HOG_319</th>\n",
       "      <th>HOG_320</th>\n",
       "      <th>HOG_321</th>\n",
       "      <th>HOG_322</th>\n",
       "      <th>HOG_323</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.164556</td>\n",
       "      <td>0.110298</td>\n",
       "      <td>0.114931</td>\n",
       "      <td>0.271179</td>\n",
       "      <td>0.169717</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>0.032058</td>\n",
       "      <td>0.128274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178965</td>\n",
       "      <td>0.071544</td>\n",
       "      <td>0.034731</td>\n",
       "      <td>0.160832</td>\n",
       "      <td>0.215133</td>\n",
       "      <td>0.077233</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.174348</td>\n",
       "      <td>0.053133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042332</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>0.186208</td>\n",
       "      <td>0.458941</td>\n",
       "      <td>0.127396</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>0.044536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.058476</td>\n",
       "      <td>0.338194</td>\n",
       "      <td>0.090360</td>\n",
       "      <td>0.037042</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171644</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.085235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293204</td>\n",
       "      <td>0.312434</td>\n",
       "      <td>0.416592</td>\n",
       "      <td>0.135259</td>\n",
       "      <td>0.200463</td>\n",
       "      <td>0.170252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116966</td>\n",
       "      <td>0.040826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157485</td>\n",
       "      <td>0.174811</td>\n",
       "      <td>0.229987</td>\n",
       "      <td>0.108690</td>\n",
       "      <td>0.194863</td>\n",
       "      <td>0.189121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039423</td>\n",
       "      <td>0.076545</td>\n",
       "      <td>0.318370</td>\n",
       "      <td>0.238697</td>\n",
       "      <td>0.182499</td>\n",
       "      <td>0.100955</td>\n",
       "      <td>0.084661</td>\n",
       "      <td>0.228457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>0.154246</td>\n",
       "      <td>0.773984</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.052806</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.244265</td>\n",
       "      <td>0.088502</td>\n",
       "      <td>0.200047</td>\n",
       "      <td>0.125870</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154710</td>\n",
       "      <td>0.076618</td>\n",
       "      <td>0.048245</td>\n",
       "      <td>0.139931</td>\n",
       "      <td>0.181676</td>\n",
       "      <td>0.019553</td>\n",
       "      <td>0.061611</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>0.039013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HOG_0     HOG_1     HOG_2     HOG_3     HOG_4     HOG_5     HOG_6  \\\n",
       "0  0.164556  0.110298  0.114931  0.271179  0.169717  0.054026  0.000000   \n",
       "1  0.042332  0.014491  0.008546  0.186208  0.458941  0.127396  0.026936   \n",
       "2  0.171644  0.030002  0.085235  0.000000  0.293204  0.312434  0.416592   \n",
       "3  0.137398  0.000000  0.039423  0.076545  0.318370  0.238697  0.182499   \n",
       "4  0.052806  0.023038  0.244265  0.088502  0.200047  0.125870  0.030002   \n",
       "\n",
       "      HOG_7     HOG_8     HOG_9  ...   HOG_315   HOG_316   HOG_317   HOG_318  \\\n",
       "0  0.013909  0.032058  0.128274  ...  0.178965  0.071544  0.034731  0.160832   \n",
       "1  0.002414  0.010556  0.044536  ...  0.016068  0.005337  0.001035  0.058476   \n",
       "2  0.135259  0.200463  0.170252  ...  0.116966  0.040826  0.000000  0.157485   \n",
       "3  0.100955  0.084661  0.228457  ...  0.025416  0.000000  0.005985  0.154246   \n",
       "4  0.026564  0.000000  0.116017  ...  0.154710  0.076618  0.048245  0.139931   \n",
       "\n",
       "    HOG_319   HOG_320   HOG_321   HOG_322   HOG_323  label  \n",
       "0  0.215133  0.077233  0.080584  0.174348  0.053133      0  \n",
       "1  0.338194  0.090360  0.037042  0.018520  0.009288      0  \n",
       "2  0.174811  0.229987  0.108690  0.194863  0.189121      0  \n",
       "3  0.773984  0.022197  0.002287  0.006587  0.000000      0  \n",
       "4  0.181676  0.019553  0.061611  0.012055  0.039013      0  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hog_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc041b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOG_0</th>\n",
       "      <th>HOG_1</th>\n",
       "      <th>HOG_2</th>\n",
       "      <th>HOG_3</th>\n",
       "      <th>HOG_4</th>\n",
       "      <th>HOG_5</th>\n",
       "      <th>HOG_6</th>\n",
       "      <th>HOG_7</th>\n",
       "      <th>HOG_8</th>\n",
       "      <th>HOG_9</th>\n",
       "      <th>...</th>\n",
       "      <th>HOG_315</th>\n",
       "      <th>HOG_316</th>\n",
       "      <th>HOG_317</th>\n",
       "      <th>HOG_318</th>\n",
       "      <th>HOG_319</th>\n",
       "      <th>HOG_320</th>\n",
       "      <th>HOG_321</th>\n",
       "      <th>HOG_322</th>\n",
       "      <th>HOG_323</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.133233</td>\n",
       "      <td>0.147257</td>\n",
       "      <td>0.155707</td>\n",
       "      <td>0.247196</td>\n",
       "      <td>0.234899</td>\n",
       "      <td>0.161379</td>\n",
       "      <td>0.039895</td>\n",
       "      <td>0.113214</td>\n",
       "      <td>0.084901</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221702</td>\n",
       "      <td>0.220815</td>\n",
       "      <td>0.207102</td>\n",
       "      <td>0.148685</td>\n",
       "      <td>0.139678</td>\n",
       "      <td>0.046422</td>\n",
       "      <td>0.103002</td>\n",
       "      <td>0.022021</td>\n",
       "      <td>0.095391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.035142</td>\n",
       "      <td>0.322201</td>\n",
       "      <td>0.252297</td>\n",
       "      <td>0.047138</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.064328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064858</td>\n",
       "      <td>0.141359</td>\n",
       "      <td>0.082815</td>\n",
       "      <td>0.041605</td>\n",
       "      <td>0.027038</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.108954</td>\n",
       "      <td>0.021148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102442</td>\n",
       "      <td>0.017749</td>\n",
       "      <td>0.145625</td>\n",
       "      <td>0.067538</td>\n",
       "      <td>0.116081</td>\n",
       "      <td>0.014466</td>\n",
       "      <td>0.020879</td>\n",
       "      <td>0.016245</td>\n",
       "      <td>0.017870</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162728</td>\n",
       "      <td>0.056431</td>\n",
       "      <td>0.104525</td>\n",
       "      <td>0.113858</td>\n",
       "      <td>0.246311</td>\n",
       "      <td>0.130088</td>\n",
       "      <td>0.113036</td>\n",
       "      <td>0.048274</td>\n",
       "      <td>0.146711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.126457</td>\n",
       "      <td>0.061866</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.056195</td>\n",
       "      <td>0.104592</td>\n",
       "      <td>0.047203</td>\n",
       "      <td>0.015626</td>\n",
       "      <td>0.116695</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.178866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227151</td>\n",
       "      <td>0.197967</td>\n",
       "      <td>0.370338</td>\n",
       "      <td>0.180816</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.047681</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.114480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.198966</td>\n",
       "      <td>0.074091</td>\n",
       "      <td>0.031344</td>\n",
       "      <td>0.263441</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.199880</td>\n",
       "      <td>0.073618</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>0.069013</td>\n",
       "      <td>0.186227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147104</td>\n",
       "      <td>0.090647</td>\n",
       "      <td>0.060828</td>\n",
       "      <td>0.107004</td>\n",
       "      <td>0.084967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>0.015751</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HOG_0     HOG_1     HOG_2     HOG_3     HOG_4     HOG_5     HOG_6  \\\n",
       "0  0.133233  0.147257  0.155707  0.247196  0.234899  0.161379  0.039895   \n",
       "1  0.059590  0.010019  0.035142  0.322201  0.252297  0.047138  0.013749   \n",
       "2  0.102442  0.017749  0.145625  0.067538  0.116081  0.014466  0.020879   \n",
       "3  0.126457  0.061866  0.000678  0.056195  0.104592  0.047203  0.015626   \n",
       "4  0.198966  0.074091  0.031344  0.263441  0.384085  0.199880  0.073618   \n",
       "\n",
       "      HOG_7     HOG_8     HOG_9  ...   HOG_315   HOG_316   HOG_317   HOG_318  \\\n",
       "0  0.113214  0.084901  0.012819  ...  0.221702  0.220815  0.207102  0.148685   \n",
       "1  0.005209  0.004802  0.064328  ...  0.064858  0.141359  0.082815  0.041605   \n",
       "2  0.016245  0.017870  0.038912  ...  0.162728  0.056431  0.104525  0.113858   \n",
       "3  0.116695  0.055012  0.178866  ...  0.227151  0.197967  0.370338  0.180816   \n",
       "4  0.047190  0.069013  0.186227  ...  0.147104  0.090647  0.060828  0.107004   \n",
       "\n",
       "    HOG_319   HOG_320   HOG_321   HOG_322   HOG_323  label  \n",
       "0  0.139678  0.046422  0.103002  0.022021  0.095391      0  \n",
       "1  0.027038  0.058822  0.060822  0.108954  0.021148      0  \n",
       "2  0.246311  0.130088  0.113036  0.048274  0.146711      0  \n",
       "3  0.171524  0.047681  0.013201  0.034434  0.114480      0  \n",
       "4  0.084967  0.000000  0.016345  0.015751  0.055143      0  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hog_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a225435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.035235</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.017255</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010986</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.012773</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.013296</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016285</td>\n",
       "      <td>0.012491</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.017579</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.011420</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010716</td>\n",
       "      <td>0.013054</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0            0.025487           0.015180             0.015745   \n",
       "1            0.010986           0.012186             0.012773   \n",
       "2            0.016285           0.012491             0.013393   \n",
       "3            0.010716           0.013054             0.010267   \n",
       "4            0.010842           0.008827             0.007563   \n",
       "\n",
       "   gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0            0.008965             0.035235            0.023189   \n",
       "1            0.015618             0.022050            0.013359   \n",
       "2            0.008042             0.017579            0.009739   \n",
       "3            0.008752             0.022262            0.023039   \n",
       "4            0.004795             0.013905            0.008357   \n",
       "\n",
       "   gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0              0.017233             0.009223            0.014462   \n",
       "1              0.015124             0.013296            0.008714   \n",
       "2              0.013057             0.007870            0.011420   \n",
       "3              0.009274             0.010089            0.005055   \n",
       "4              0.008254             0.007586            0.009359   \n",
       "\n",
       "   gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0           0.010695             0.010592            0.006646   \n",
       "1           0.012276             0.006274            0.007100   \n",
       "2           0.009930             0.008178            0.004998   \n",
       "3           0.005926             0.004737            0.004743   \n",
       "4           0.007253             0.006268            0.003545   \n",
       "\n",
       "   gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0             0.021182            0.017255              0.012010   \n",
       "1             0.011564            0.008165              0.009103   \n",
       "2             0.009171            0.005822              0.009335   \n",
       "3             0.010330            0.011244              0.004923   \n",
       "4             0.006657            0.004499              0.005974   \n",
       "\n",
       "   gabor_f0.4_t135_std  label  \n",
       "0             0.007830      0  \n",
       "1             0.009235      0  \n",
       "2             0.006005      0  \n",
       "3             0.005105      0  \n",
       "4             0.003613      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gabor_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfafde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.027085</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.015226</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>0.015012</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016755</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.020906</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.015775</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>0.012243</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>0.007593</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0            0.019114           0.011690             0.013208   \n",
       "1            0.016755           0.014683             0.020438   \n",
       "2            0.021395           0.015775             0.009892   \n",
       "3            0.018256           0.014299             0.014059   \n",
       "4            0.004899           0.003762             0.004362   \n",
       "\n",
       "   gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0            0.008110             0.027085            0.017980   \n",
       "1            0.015542             0.024632            0.020906   \n",
       "2            0.007718             0.008575            0.008443   \n",
       "3            0.010218             0.012424            0.008191   \n",
       "4            0.003657             0.006170            0.004497   \n",
       "\n",
       "   gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0              0.014299             0.007634            0.015226   \n",
       "1              0.014697             0.015717            0.010085   \n",
       "2              0.010142             0.007819            0.012059   \n",
       "3              0.012866             0.009325            0.012865   \n",
       "4              0.004168             0.003116            0.004336   \n",
       "\n",
       "   gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0           0.013386             0.010334            0.007067   \n",
       "1           0.010674             0.010974            0.010191   \n",
       "2           0.012243             0.006099            0.005641   \n",
       "3           0.012881             0.008574            0.006384   \n",
       "4           0.003628             0.002837            0.002456   \n",
       "\n",
       "   gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0             0.018516            0.015012              0.009861   \n",
       "1             0.011706            0.012159              0.008319   \n",
       "2             0.005278            0.004273              0.005680   \n",
       "3             0.007717            0.007593              0.009149   \n",
       "4             0.002710            0.002006              0.002680   \n",
       "\n",
       "   gabor_f0.4_t135_std  label  \n",
       "0             0.006252      0  \n",
       "1             0.007945      0  \n",
       "2             0.005195      0  \n",
       "3             0.007213      0  \n",
       "4             0.002148      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gabor_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f17198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_hist_0</th>\n",
       "      <th>red_hist_1</th>\n",
       "      <th>red_hist_2</th>\n",
       "      <th>red_hist_3</th>\n",
       "      <th>red_hist_4</th>\n",
       "      <th>red_hist_5</th>\n",
       "      <th>red_hist_6</th>\n",
       "      <th>red_hist_7</th>\n",
       "      <th>red_hist_8</th>\n",
       "      <th>red_hist_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.008456</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.018566</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_hist_0  red_hist_1  red_hist_2  red_hist_3  red_hist_4  red_hist_5  \\\n",
       "0    0.000919    0.001042    0.003493    0.003799    0.005576    0.007292   \n",
       "1    0.004044    0.003309    0.004902    0.009681    0.011213    0.008150   \n",
       "2    0.000245    0.000919    0.001716    0.003248    0.004963    0.007475   \n",
       "3    0.000613    0.018566    0.007966    0.006005    0.004534    0.003676   \n",
       "4    0.000000    0.000306    0.002512    0.003370    0.009191    0.018934   \n",
       "\n",
       "   red_hist_6  red_hist_7  red_hist_8  red_hist_9  ...  hsv_h7_s1_v3  \\\n",
       "0    0.005208    0.004657    0.004779    0.004596  ...           0.0   \n",
       "1    0.003248    0.001961    0.002145    0.003125  ...           0.0   \n",
       "2    0.008578    0.007475    0.008456    0.006495  ...           0.0   \n",
       "3    0.003922    0.004534    0.002757    0.001593  ...           0.0   \n",
       "4    0.012255    0.007475    0.002880    0.001103  ...           0.0   \n",
       "\n",
       "   hsv_h7_s2_v0  hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  \\\n",
       "0      0.000000           0.0           0.0           0.0      0.000000   \n",
       "1      0.000977           0.0           0.0           0.0      0.001953   \n",
       "2      0.000000           0.0           0.0           0.0      0.000000   \n",
       "3      0.000000           0.0           0.0           0.0      0.000000   \n",
       "4      0.000000           0.0           0.0           0.0      0.000000   \n",
       "\n",
       "   hsv_h7_s3_v1  hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0           0.0           0.0           0.0      0  \n",
       "1           0.0           0.0           0.0      0  \n",
       "2           0.0           0.0           0.0      0  \n",
       "3           0.0           0.0           0.0      0  \n",
       "4           0.0           0.0           0.0      0  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_color_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af23bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_hist_0</th>\n",
       "      <th>red_hist_1</th>\n",
       "      <th>red_hist_2</th>\n",
       "      <th>red_hist_3</th>\n",
       "      <th>red_hist_4</th>\n",
       "      <th>red_hist_5</th>\n",
       "      <th>red_hist_6</th>\n",
       "      <th>red_hist_7</th>\n",
       "      <th>red_hist_8</th>\n",
       "      <th>red_hist_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.008885</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.024449</td>\n",
       "      <td>0.027390</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_hist_0  red_hist_1  red_hist_2  red_hist_3  red_hist_4  red_hist_5  \\\n",
       "0    0.000429    0.003431    0.003248    0.003554    0.004105    0.002574   \n",
       "1    0.005944    0.004534    0.004105    0.003799    0.004779    0.003676   \n",
       "2    0.001103    0.003983    0.004963    0.005331    0.003248    0.003186   \n",
       "3    0.000123    0.001716    0.006556    0.007598    0.004657    0.005331   \n",
       "4    0.000000    0.001164    0.024449    0.027390    0.006985    0.001961   \n",
       "\n",
       "   red_hist_6  red_hist_7  red_hist_8  red_hist_9  ...  hsv_h7_s1_v3  \\\n",
       "0    0.002757    0.004657    0.006066    0.007292  ...      0.000000   \n",
       "1    0.004167    0.004534    0.008885    0.006311  ...      0.000977   \n",
       "2    0.005086    0.011458    0.017279    0.005025  ...      0.000000   \n",
       "3    0.005453    0.006556    0.006127    0.007169  ...      0.000000   \n",
       "4    0.000613    0.000184    0.000000    0.000000  ...      0.000000   \n",
       "\n",
       "   hsv_h7_s2_v0  hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  \\\n",
       "0      0.000000      0.000000      0.000000           0.0           0.0   \n",
       "1      0.007812      0.018555      0.003906           0.0           0.0   \n",
       "2      0.006836      0.000977      0.000000           0.0           0.0   \n",
       "3      0.000000      0.000000      0.000000           0.0           0.0   \n",
       "4      0.000000      0.000000      0.000000           0.0           0.0   \n",
       "\n",
       "   hsv_h7_s3_v1  hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0      0.000000      0.000000           0.0      0  \n",
       "1      0.011719      0.003906           0.0      0  \n",
       "2      0.000000      0.000000           0.0      0  \n",
       "3      0.000000      0.000000           0.0      0  \n",
       "4      0.000000      0.000000           0.0      0  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_color_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2dfe503",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = pd.concat(\n",
    "    [\n",
    "        df_lbp_train.drop(columns=[\"label\"]),\n",
    "        df_hog_train.drop(columns=[\"label\"]),\n",
    "        df_gabor_train.drop(columns=[\"label\"]),\n",
    "        df_color_train.drop(columns=[\"label\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df_all_train[\"label\"] = df_lbp_train[\"label\"]\n",
    "df_all_test = pd.concat(\n",
    "    [\n",
    "        df_lbp_test.drop(columns=[\"label\"]),\n",
    "        df_hog_test.drop(columns=[\"label\"]),\n",
    "        df_gabor_test.drop(columns=[\"label\"]),\n",
    "        df_color_test.drop(columns=[\"label\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df_all_test[\"label\"] = df_lbp_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1838a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "LBP_ONLY_COLS = [col for col in df_lbp_train.columns if col != \"label\"]\n",
    "HOG_ONLY_COLS = [col for col in df_hog_train.columns if col != \"label\"]\n",
    "GABOR_ONLY_COLS = [col for col in df_gabor_train.columns if col != \"label\"]\n",
    "COLOR_ONLY_COLS = [col for col in df_color_train.columns if col != \"label\"]\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"LBP\": LBP_ONLY_COLS,\n",
    "    \"HOG\": HOG_ONLY_COLS,\n",
    "    \"Gabor\": GABOR_ONLY_COLS,\n",
    "    \"Color\": COLOR_ONLY_COLS,\n",
    "    \"All\": df_all_train.columns.drop(\"label\").tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114a5c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0.276367</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0      0.070312  0.095703  0.058594  0.111328  0.200195  0.151367  0.059570   \n",
       "1      0.062500  0.091797  0.062500  0.105469  0.244141  0.124023  0.046875   \n",
       "2      0.068359  0.095703  0.064453  0.126953  0.177734  0.144531  0.076172   \n",
       "3      0.048828  0.084961  0.065430  0.096680  0.247070  0.145508  0.061523   \n",
       "4      0.065430  0.103516  0.081055  0.110352  0.171875  0.118164  0.093750   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  0.053711  0.096680  0.051758  0.131836  0.244141  0.147461  0.052734   \n",
       "99996  0.041016  0.056641  0.041992  0.122070  0.276367  0.180664  0.063477   \n",
       "99997  0.068359  0.113281  0.055664  0.085938  0.205078  0.113281  0.045898   \n",
       "99998  0.044922  0.119141  0.058594  0.124023  0.193359  0.126953  0.064453   \n",
       "99999  0.043945  0.077148  0.040039  0.096680  0.174805  0.214844  0.079102   \n",
       "\n",
       "          LBP_7     LBP_8     LBP_9  ...  hsv_h7_s1_v3  hsv_h7_s2_v0  \\\n",
       "0      0.070312  0.050781  0.131836  ...           0.0      0.000000   \n",
       "1      0.073242  0.059570  0.129883  ...           0.0      0.000977   \n",
       "2      0.066406  0.054688  0.125000  ...           0.0      0.000000   \n",
       "3      0.075195  0.069336  0.105469  ...           0.0      0.000000   \n",
       "4      0.064453  0.060547  0.130859  ...           0.0      0.000000   \n",
       "...         ...       ...       ...  ...           ...           ...   \n",
       "99995  0.069336  0.053711  0.098633  ...           0.0      0.000977   \n",
       "99996  0.054688  0.068359  0.094727  ...           0.0      0.000977   \n",
       "99997  0.086914  0.088867  0.136719  ...           0.0      0.006836   \n",
       "99998  0.093750  0.050781  0.124023  ...           0.0      0.000000   \n",
       "99999  0.091797  0.083984  0.097656  ...           0.0      0.003906   \n",
       "\n",
       "       hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  hsv_h7_s3_v1  \\\n",
       "0               0.0           0.0           0.0      0.000000           0.0   \n",
       "1               0.0           0.0           0.0      0.001953           0.0   \n",
       "2               0.0           0.0           0.0      0.000000           0.0   \n",
       "3               0.0           0.0           0.0      0.000000           0.0   \n",
       "4               0.0           0.0           0.0      0.000000           0.0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "99995           0.0           0.0           0.0      0.001953           0.0   \n",
       "99996           0.0           0.0           0.0      0.000977           0.0   \n",
       "99997           0.0           0.0           0.0      0.002930           0.0   \n",
       "99998           0.0           0.0           0.0      0.000000           0.0   \n",
       "99999           0.0           0.0           0.0      0.000977           0.0   \n",
       "\n",
       "       hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0               0.0           0.0      0  \n",
       "1               0.0           0.0      0  \n",
       "2               0.0           0.0      0  \n",
       "3               0.0           0.0      0  \n",
       "4               0.0           0.0      0  \n",
       "...             ...           ...    ...  \n",
       "99995           0.0           0.0      1  \n",
       "99996           0.0           0.0      1  \n",
       "99997           0.0           0.0      1  \n",
       "99998           0.0           0.0      1  \n",
       "99999           0.0           0.0      1  \n",
       "\n",
       "[100000 rows x 527 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52527cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.035235</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.017255</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.007830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010986</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.012773</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.013296</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.009235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016285</td>\n",
       "      <td>0.012491</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.017579</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.011420</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.006005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010716</td>\n",
       "      <td>0.013054</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.005105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.003613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.023709</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>0.010362</td>\n",
       "      <td>0.018512</td>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>0.013666</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>0.015813</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.009398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>0.010681</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.014681</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.009966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.016462</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.017052</td>\n",
       "      <td>0.011470</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>0.008679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.019635</td>\n",
       "      <td>0.024341</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.017972</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>0.009530</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.013153</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>0.025456</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.006596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.025979</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.020543</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.008676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0                0.025487           0.015180             0.015745   \n",
       "1                0.010986           0.012186             0.012773   \n",
       "2                0.016285           0.012491             0.013393   \n",
       "3                0.010716           0.013054             0.010267   \n",
       "4                0.010842           0.008827             0.007563   \n",
       "...                   ...                ...                  ...   \n",
       "99995            0.025049           0.023709             0.010386   \n",
       "99996            0.007965           0.009670             0.008950   \n",
       "99997            0.022883           0.021229             0.016462   \n",
       "99998            0.019635           0.024341             0.007484   \n",
       "99999            0.004822           0.007053             0.009022   \n",
       "\n",
       "       gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0                0.008965             0.035235            0.023189   \n",
       "1                0.015618             0.022050            0.013359   \n",
       "2                0.008042             0.017579            0.009739   \n",
       "3                0.008752             0.022262            0.023039   \n",
       "4                0.004795             0.013905            0.008357   \n",
       "...                   ...                  ...                 ...   \n",
       "99995            0.010362             0.018512            0.016688   \n",
       "99996            0.012678             0.008978            0.010681   \n",
       "99997            0.017172             0.017052            0.011470   \n",
       "99998            0.009505             0.017972            0.023331   \n",
       "99999            0.013302             0.016100            0.025979   \n",
       "\n",
       "       gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0                  0.017233             0.009223            0.014462   \n",
       "1                  0.015124             0.013296            0.008714   \n",
       "2                  0.013057             0.007870            0.011420   \n",
       "3                  0.009274             0.010089            0.005055   \n",
       "4                  0.008254             0.007586            0.009359   \n",
       "...                     ...                  ...                 ...   \n",
       "99995              0.012819             0.014602            0.014546   \n",
       "99996              0.011538             0.014681            0.004960   \n",
       "99997              0.013510             0.011879            0.013327   \n",
       "99998              0.009134             0.009530            0.010233   \n",
       "99999              0.004746             0.006832            0.006472   \n",
       "\n",
       "       gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0               0.010695             0.010592            0.006646   \n",
       "1               0.012276             0.006274            0.007100   \n",
       "2               0.009930             0.008178            0.004998   \n",
       "3               0.005926             0.004737            0.004743   \n",
       "4               0.007253             0.006268            0.003545   \n",
       "...                  ...                  ...                 ...   \n",
       "99995           0.013666             0.006869            0.006157   \n",
       "99996           0.007368             0.005335            0.007723   \n",
       "99997           0.015917             0.010689            0.014874   \n",
       "99998           0.013153             0.004244            0.005433   \n",
       "99999           0.012898             0.006458            0.010627   \n",
       "\n",
       "       gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0                 0.021182            0.017255              0.012010   \n",
       "1                 0.011564            0.008165              0.009103   \n",
       "2                 0.009171            0.005822              0.009335   \n",
       "3                 0.010330            0.011244              0.004923   \n",
       "4                 0.006657            0.004499              0.005974   \n",
       "...                    ...                 ...                   ...   \n",
       "99995             0.013893            0.015813              0.008002   \n",
       "99996             0.007333            0.011752              0.006975   \n",
       "99997             0.013905            0.009374              0.008678   \n",
       "99998             0.016788            0.025456              0.005581   \n",
       "99999             0.011596            0.020543              0.005488   \n",
       "\n",
       "       gabor_f0.4_t135_std  \n",
       "0                 0.007830  \n",
       "1                 0.009235  \n",
       "2                 0.006005  \n",
       "3                 0.005105  \n",
       "4                 0.003613  \n",
       "...                    ...  \n",
       "99995             0.009398  \n",
       "99996             0.009966  \n",
       "99997             0.008679  \n",
       "99998             0.006596  \n",
       "99999             0.008676  \n",
       "\n",
       "[100000 rows x 16 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train[GABOR_ONLY_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b808ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBP_0</th>\n",
       "      <th>LBP_1</th>\n",
       "      <th>LBP_2</th>\n",
       "      <th>LBP_3</th>\n",
       "      <th>LBP_4</th>\n",
       "      <th>LBP_5</th>\n",
       "      <th>LBP_6</th>\n",
       "      <th>LBP_7</th>\n",
       "      <th>LBP_8</th>\n",
       "      <th>LBP_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv_h7_s1_v3</th>\n",
       "      <th>hsv_h7_s2_v0</th>\n",
       "      <th>hsv_h7_s2_v1</th>\n",
       "      <th>hsv_h7_s2_v2</th>\n",
       "      <th>hsv_h7_s2_v3</th>\n",
       "      <th>hsv_h7_s3_v0</th>\n",
       "      <th>hsv_h7_s3_v1</th>\n",
       "      <th>hsv_h7_s3_v2</th>\n",
       "      <th>hsv_h7_s3_v3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.092773</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LBP_0     LBP_1     LBP_2     LBP_3     LBP_4     LBP_5     LBP_6  \\\n",
       "0      0.074219  0.099609  0.061523  0.117188  0.161133  0.127930  0.076172   \n",
       "1      0.075195  0.092773  0.057617  0.117188  0.190430  0.128906  0.072266   \n",
       "2      0.067383  0.082031  0.067383  0.130859  0.201172  0.138672  0.073242   \n",
       "3      0.063477  0.105469  0.073242  0.111328  0.190430  0.112305  0.050781   \n",
       "4      0.039062  0.086914  0.069336  0.161133  0.194336  0.159180  0.078125   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995  0.069336  0.099609  0.061523  0.100586  0.199219  0.133789  0.052734   \n",
       "19996  0.078125  0.111328  0.055664  0.109375  0.181641  0.109375  0.060547   \n",
       "19997  0.062500  0.076172  0.083984  0.127930  0.237305  0.129883  0.055664   \n",
       "19998  0.043945  0.094727  0.071289  0.106445  0.246094  0.154297  0.056641   \n",
       "19999  0.043945  0.100586  0.039062  0.094727  0.246094  0.213867  0.036133   \n",
       "\n",
       "          LBP_7     LBP_8     LBP_9  ...  hsv_h7_s1_v3  hsv_h7_s2_v0  \\\n",
       "0      0.076172  0.058594  0.147461  ...      0.000000      0.000000   \n",
       "1      0.062500  0.067383  0.135742  ...      0.000977      0.007812   \n",
       "2      0.068359  0.050781  0.120117  ...      0.000000      0.006836   \n",
       "3      0.090820  0.069336  0.132812  ...      0.000000      0.000000   \n",
       "4      0.071289  0.049805  0.090820  ...      0.000000      0.000000   \n",
       "...         ...       ...       ...  ...           ...           ...   \n",
       "19995  0.083008  0.061523  0.138672  ...      0.000000      0.000000   \n",
       "19996  0.085938  0.074219  0.133789  ...      0.000000      0.000000   \n",
       "19997  0.067383  0.055664  0.103516  ...      0.000000      0.003906   \n",
       "19998  0.083984  0.045898  0.096680  ...      0.000000      0.000000   \n",
       "19999  0.069336  0.054688  0.101562  ...      0.000000      0.000000   \n",
       "\n",
       "       hsv_h7_s2_v1  hsv_h7_s2_v2  hsv_h7_s2_v3  hsv_h7_s3_v0  hsv_h7_s3_v1  \\\n",
       "0          0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "1          0.018555      0.003906           0.0      0.000000      0.011719   \n",
       "2          0.000977      0.000000           0.0      0.000000      0.000000   \n",
       "3          0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "4          0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "19995      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "19996      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "19997      0.000000      0.000000           0.0      0.000977      0.000000   \n",
       "19998      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "19999      0.000000      0.000000           0.0      0.000000      0.000000   \n",
       "\n",
       "       hsv_h7_s3_v2  hsv_h7_s3_v3  label  \n",
       "0          0.000000           0.0      0  \n",
       "1          0.003906           0.0      0  \n",
       "2          0.000000           0.0      0  \n",
       "3          0.000000           0.0      0  \n",
       "4          0.000000           0.0      0  \n",
       "...             ...           ...    ...  \n",
       "19995      0.000000           0.0      1  \n",
       "19996      0.000000           0.0      1  \n",
       "19997      0.000000           0.0      1  \n",
       "19998      0.000000           0.0      1  \n",
       "19999      0.000000           0.0      1  \n",
       "\n",
       "[20000 rows x 527 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e399ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gabor_f0.2_t0_mean</th>\n",
       "      <th>gabor_f0.2_t0_std</th>\n",
       "      <th>gabor_f0.2_t45_mean</th>\n",
       "      <th>gabor_f0.2_t45_std</th>\n",
       "      <th>gabor_f0.2_t90_mean</th>\n",
       "      <th>gabor_f0.2_t90_std</th>\n",
       "      <th>gabor_f0.2_t135_mean</th>\n",
       "      <th>gabor_f0.2_t135_std</th>\n",
       "      <th>gabor_f0.4_t0_mean</th>\n",
       "      <th>gabor_f0.4_t0_std</th>\n",
       "      <th>gabor_f0.4_t45_mean</th>\n",
       "      <th>gabor_f0.4_t45_std</th>\n",
       "      <th>gabor_f0.4_t90_mean</th>\n",
       "      <th>gabor_f0.4_t90_std</th>\n",
       "      <th>gabor_f0.4_t135_mean</th>\n",
       "      <th>gabor_f0.4_t135_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.027085</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.015226</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>0.015012</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.006252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016755</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.020906</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.007945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.015775</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>0.012243</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.005195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>0.007593</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.007213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.002148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.014144</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.012557</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.017888</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.011663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.023224</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.010277</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.010091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.024592</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.028831</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.019510</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.015365</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.011870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.018583</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.020778</td>\n",
       "      <td>0.030662</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.007055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gabor_f0.2_t0_mean  gabor_f0.2_t0_std  gabor_f0.2_t45_mean  \\\n",
       "0                0.019114           0.011690             0.013208   \n",
       "1                0.016755           0.014683             0.020438   \n",
       "2                0.021395           0.015775             0.009892   \n",
       "3                0.018256           0.014299             0.014059   \n",
       "4                0.004899           0.003762             0.004362   \n",
       "...                   ...                ...                  ...   \n",
       "19995            0.014299           0.014697             0.014144   \n",
       "19996            0.015771           0.015738             0.012362   \n",
       "19997            0.024592           0.025211             0.014526   \n",
       "19998            0.004713           0.004573             0.005187   \n",
       "19999            0.005310           0.006827             0.005772   \n",
       "\n",
       "       gabor_f0.2_t45_std  gabor_f0.2_t90_mean  gabor_f0.2_t90_std  \\\n",
       "0                0.008110             0.027085            0.017980   \n",
       "1                0.015542             0.024632            0.020906   \n",
       "2                0.007718             0.008575            0.008443   \n",
       "3                0.010218             0.012424            0.008191   \n",
       "4                0.003657             0.006170            0.004497   \n",
       "...                   ...                  ...                 ...   \n",
       "19995            0.013913             0.033609            0.022614   \n",
       "19996            0.010676             0.023224            0.016746   \n",
       "19997            0.012060             0.028831            0.018488   \n",
       "19998            0.004802             0.016036            0.016051   \n",
       "19999            0.006579             0.016060            0.018583   \n",
       "\n",
       "       gabor_f0.2_t135_mean  gabor_f0.2_t135_std  gabor_f0.4_t0_mean  \\\n",
       "0                  0.014299             0.007634            0.015226   \n",
       "1                  0.014697             0.015717            0.010085   \n",
       "2                  0.010142             0.007819            0.012059   \n",
       "3                  0.012866             0.009325            0.012865   \n",
       "4                  0.004168             0.003116            0.004336   \n",
       "...                     ...                  ...                 ...   \n",
       "19995              0.013248             0.013862            0.010164   \n",
       "19996              0.018841             0.016119            0.009160   \n",
       "19997              0.015063             0.012784            0.014732   \n",
       "19998              0.005165             0.004952            0.003121   \n",
       "19999              0.006342             0.007145            0.007624   \n",
       "\n",
       "       gabor_f0.4_t0_std  gabor_f0.4_t45_mean  gabor_f0.4_t45_std  \\\n",
       "0               0.013386             0.010334            0.007067   \n",
       "1               0.010674             0.010974            0.010191   \n",
       "2               0.012243             0.006099            0.005641   \n",
       "3               0.012881             0.008574            0.006384   \n",
       "4               0.003628             0.002837            0.002456   \n",
       "...                  ...                  ...                 ...   \n",
       "19995           0.012557             0.008433            0.010102   \n",
       "19996           0.011407             0.010277            0.010375   \n",
       "19997           0.019510             0.009216            0.009262   \n",
       "19998           0.005692             0.002702            0.002666   \n",
       "19999           0.014423             0.005144            0.006877   \n",
       "\n",
       "       gabor_f0.4_t90_mean  gabor_f0.4_t90_std  gabor_f0.4_t135_mean  \\\n",
       "0                 0.018516            0.015012              0.009861   \n",
       "1                 0.011706            0.012159              0.008319   \n",
       "2                 0.005278            0.004273              0.005680   \n",
       "3                 0.007717            0.007593              0.009149   \n",
       "4                 0.002710            0.002006              0.002680   \n",
       "...                    ...                 ...                   ...   \n",
       "19995             0.017888            0.013578              0.009319   \n",
       "19996             0.019426            0.017686              0.010390   \n",
       "19997             0.015365            0.012708              0.010608   \n",
       "19998             0.009823            0.011978              0.002969   \n",
       "19999             0.020778            0.030662              0.005206   \n",
       "\n",
       "       gabor_f0.4_t135_std  \n",
       "0                 0.006252  \n",
       "1                 0.007945  \n",
       "2                 0.005195  \n",
       "3                 0.007213  \n",
       "4                 0.002148  \n",
       "...                    ...  \n",
       "19995             0.011663  \n",
       "19996             0.010091  \n",
       "19997             0.011870  \n",
       "19998             0.003287  \n",
       "19999             0.007055  \n",
       "\n",
       "[20000 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_test[GABOR_ONLY_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d62f5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running experiments for feature set: LBP\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.01 seconds for 100000 samples\n",
      "Prediction time: 0.00 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6131    0.7958    0.6926     10000\n",
      "        Fake     0.7091    0.4978    0.5850     10000\n",
      "\n",
      "    accuracy                         0.6468     20000\n",
      "   macro avg     0.6611    0.6468    0.6388     20000\n",
      "weighted avg     0.6611    0.6468    0.6388     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.10 seconds for 100000 samples\n",
      "Prediction time: 1.21 seconds for 20000 samples\n",
      "Model size (joblib): 2.578 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6718    0.6904    0.6810     10000\n",
      "        Fake     0.6816    0.6627    0.6720     10000\n",
      "\n",
      "    accuracy                         0.6765     20000\n",
      "   macro avg     0.6767    0.6765    0.6765     20000\n",
      "weighted avg     0.6767    0.6765    0.6765     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 8.23 seconds for 100000 samples\n",
      "Prediction time: 0.00 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6749    0.7269    0.7000     10000\n",
      "        Fake     0.7041    0.6499    0.6759     10000\n",
      "\n",
      "    accuracy                         0.6884     20000\n",
      "   macro avg     0.6895    0.6884    0.6879     20000\n",
      "weighted avg     0.6895    0.6884    0.6879     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 1.38 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 25.193 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7055    0.7509    0.7275     10000\n",
      "        Fake     0.7338    0.6866    0.7094     10000\n",
      "\n",
      "    accuracy                         0.7188     20000\n",
      "   macro avg     0.7197    0.7188    0.7185     20000\n",
      "weighted avg     0.7197    0.7188    0.7185     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 0.15 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.130 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7066    0.7492    0.7273     10000\n",
      "        Fake     0.7331    0.6889    0.7103     10000\n",
      "\n",
      "    accuracy                         0.7190     20000\n",
      "   macro avg     0.7198    0.7190    0.7188     20000\n",
      "weighted avg     0.7198    0.7190    0.7188     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: HOG\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.17 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 0.006 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6470    0.6584    0.6527     10000\n",
      "        Fake     0.6523    0.6408    0.6465     10000\n",
      "\n",
      "    accuracy                         0.6496     20000\n",
      "   macro avg     0.6496    0.6496    0.6496     20000\n",
      "weighted avg     0.6496    0.6496    0.6496     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.05 seconds for 100000 samples\n",
      "Prediction time: 6.47 seconds for 20000 samples\n",
      "Model size (joblib): 108.273 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7333    0.7715    0.7519     10000\n",
      "        Fake     0.7589    0.7194    0.7386     10000\n",
      "\n",
      "    accuracy                         0.7454     20000\n",
      "   macro avg     0.7461    0.7454    0.7453     20000\n",
      "weighted avg     0.7461    0.7454    0.7453     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 5.85 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.003 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7137    0.7044    0.7090     10000\n",
      "        Fake     0.7082    0.7174    0.7128     10000\n",
      "\n",
      "    accuracy                         0.7109     20000\n",
      "   macro avg     0.7109    0.7109    0.7109     20000\n",
      "weighted avg     0.7109    0.7109    0.7109     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.59 seconds for 100000 samples\n",
      "Prediction time: 0.08 seconds for 20000 samples\n",
      "Model size (joblib): 20.033 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7351    0.7660    0.7502     10000\n",
      "        Fake     0.7557    0.7240    0.7395     10000\n",
      "\n",
      "    accuracy                         0.7450     20000\n",
      "   macro avg     0.7454    0.7450    0.7449     20000\n",
      "weighted avg     0.7454    0.7450    0.7449     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 2.60 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.152 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7613    0.7894    0.7751     10000\n",
      "        Fake     0.7813    0.7525    0.7666     10000\n",
      "\n",
      "    accuracy                         0.7710     20000\n",
      "   macro avg     0.7713    0.7710    0.7709     20000\n",
      "weighted avg     0.7713    0.7710    0.7709     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: Gabor\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.01 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6685    0.8013    0.7289     10000\n",
      "        Fake     0.7520    0.6026    0.6691     10000\n",
      "\n",
      "    accuracy                         0.7019     20000\n",
      "   macro avg     0.7103    0.7020    0.6990     20000\n",
      "weighted avg     0.7103    0.7019    0.6990     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.00 seconds for 100000 samples\n",
      "Prediction time: 0.76 seconds for 20000 samples\n",
      "Model size (joblib): 5.281 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8308    0.8282    0.8295     10000\n",
      "        Fake     0.8287    0.8313    0.8300     10000\n",
      "\n",
      "    accuracy                         0.8297     20000\n",
      "   macro avg     0.8298    0.8297    0.8297     20000\n",
      "weighted avg     0.8298    0.8297    0.8297     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 3.53 seconds for 100000 samples\n",
      "Prediction time: 0.00 seconds for 20000 samples\n",
      "Model size (joblib): 0.001 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7129    0.7792    0.7446     10000\n",
      "        Fake     0.7566    0.6862    0.7197     10000\n",
      "\n",
      "    accuracy                         0.7327     20000\n",
      "   macro avg     0.7347    0.7327    0.7321     20000\n",
      "weighted avg     0.7347    0.7327    0.7321     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 3.80 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 16.931 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8682    0.8513    0.8597     10000\n",
      "        Fake     0.8541    0.8708    0.8624     10000\n",
      "\n",
      "    accuracy                         0.8610     20000\n",
      "   macro avg     0.8612    0.8610    0.8610     20000\n",
      "weighted avg     0.8612    0.8610    0.8610     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 0.19 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.141 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8686    0.8461    0.8572     10000\n",
      "        Fake     0.8500    0.8720    0.8609     10000\n",
      "\n",
      "    accuracy                         0.8590     20000\n",
      "   macro avg     0.8593    0.8590    0.8590     20000\n",
      "weighted avg     0.8593    0.8590    0.8590     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: Color\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.09 seconds for 100000 samples\n",
      "Prediction time: 0.04 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7589    0.4818    0.5894     10000\n",
      "        Fake     0.6204    0.8469    0.7162     10000\n",
      "\n",
      "    accuracy                         0.6643     20000\n",
      "   macro avg     0.6896    0.6643    0.6528     20000\n",
      "weighted avg     0.6896    0.6643    0.6528     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.02 seconds for 100000 samples\n",
      "Prediction time: 2.45 seconds for 20000 samples\n",
      "Model size (joblib): 11.931 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7671    0.7735    0.7703     10000\n",
      "        Fake     0.7716    0.7652    0.7684     10000\n",
      "\n",
      "    accuracy                         0.7693     20000\n",
      "   macro avg     0.7694    0.7693    0.7693     20000\n",
      "weighted avg     0.7694    0.7693    0.7693     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 3.71 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.002 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7299    0.7968    0.7619     10000\n",
      "        Fake     0.7763    0.7052    0.7390     10000\n",
      "\n",
      "    accuracy                         0.7510     20000\n",
      "   macro avg     0.7531    0.7510    0.7505     20000\n",
      "weighted avg     0.7531    0.7510    0.7505     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 4.25 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 17.944 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8290    0.8482    0.8385     10000\n",
      "        Fake     0.8446    0.8251    0.8347     10000\n",
      "\n",
      "    accuracy                         0.8367     20000\n",
      "   macro avg     0.8368    0.8366    0.8366     20000\n",
      "weighted avg     0.8368    0.8367    0.8366     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 0.99 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.135 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8320    0.8423    0.8371     10000\n",
      "        Fake     0.8403    0.8299    0.8351     10000\n",
      "\n",
      "    accuracy                         0.8361     20000\n",
      "   macro avg     0.8362    0.8361    0.8361     20000\n",
      "weighted avg     0.8362    0.8361    0.8361     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.31 seconds for 100000 samples\n",
      "Prediction time: 0.13 seconds for 20000 samples\n",
      "Model size (joblib): 0.009 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8160    0.6886    0.7469     10000\n",
      "        Fake     0.7306    0.8447    0.7835     10000\n",
      "\n",
      "    accuracy                         0.7667     20000\n",
      "   macro avg     0.7733    0.7667    0.7652     20000\n",
      "weighted avg     0.7733    0.7667    0.7652     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 6.04 seconds for 20000 samples\n",
      "Model size (joblib): 131.268 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7562    0.7815    0.7687     10000\n",
      "        Fake     0.7739    0.7481    0.7608     10000\n",
      "\n",
      "    accuracy                         0.7648     20000\n",
      "   macro avg     0.7651    0.7648    0.7647     20000\n",
      "weighted avg     0.7651    0.7648    0.7647     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 10.05 seconds for 100000 samples\n",
      "Prediction time: 0.04 seconds for 20000 samples\n",
      "Model size (joblib): 0.005 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8563    0.8762    0.8662     10000\n",
      "        Fake     0.8733    0.8530    0.8630     10000\n",
      "\n",
      "    accuracy                         0.8646     20000\n",
      "   macro avg     0.8648    0.8646    0.8646     20000\n",
      "weighted avg     0.8648    0.8646    0.8646     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.37 seconds for 100000 samples\n",
      "Prediction time: 0.08 seconds for 20000 samples\n",
      "Model size (joblib): 13.777 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9124    0.9169    0.9147     10000\n",
      "        Fake     0.9165    0.9120    0.9142     10000\n",
      "\n",
      "    accuracy                         0.9144     20000\n",
      "   macro avg     0.9145    0.9144    0.9144     20000\n",
      "weighted avg     0.9145    0.9144    0.9144     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 4.19 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.147 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9342    0.9335    0.9338     10000\n",
      "        Fake     0.9335    0.9342    0.9339     10000\n",
      "\n",
      "    accuracy                         0.9338     20000\n",
      "   macro avg     0.9339    0.9339    0.9338     20000\n",
      "weighted avg     0.9339    0.9338    0.9338     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature_set_name, feature_cols in FEATURE_SETS.items():\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Running experiments for feature set: {feature_set_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    X_train = df_all_train[feature_cols]\n",
    "    X_test = df_all_test[feature_cols]\n",
    "    y_train = df_all_train[\"label\"]\n",
    "    y_test = df_all_test[\"label\"]\n",
    "    X_train, y_train = shuffle_indexes(X_train, y_train)\n",
    "    run_ml_experiments(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        path_output=Path(\"outputs/\"),\n",
    "        feature_set_name=feature_set_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d2feaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = joblib.load(\"outputs/models/All/LGBMClassifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22189b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9342    0.9335    0.9338     10000\n",
      "        Fake     0.9335    0.9342    0.9339     10000\n",
      "\n",
      "    accuracy                         0.9338     20000\n",
      "   macro avg     0.9339    0.9339    0.9338     20000\n",
      "weighted avg     0.9339    0.9338    0.9338     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, best_model.predict(X_test), target_names=[\"Real\", \"Fake\"], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc70c457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "# Get indices of top 20 importances\n",
    "top_idx = np.argsort(importances)[-40:]\n",
    "top_features = [feature_cols[i] for i in top_idx]\n",
    "top_importances = importances[top_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_features, top_importances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "448ccbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SETS_ABLATION = {\n",
    "    \"Gabor+Color\": GABOR_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noLBP\": HOG_ONLY_COLS + GABOR_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noHOG\": LBP_ONLY_COLS + GABOR_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noGabor\": LBP_ONLY_COLS + HOG_ONLY_COLS + COLOR_ONLY_COLS,\n",
    "    \"All_noColor\": LBP_ONLY_COLS + HOG_ONLY_COLS + GABOR_ONLY_COLS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff44f8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running experiments for feature set: Gabor+Color\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7914    0.5090    0.6195     10000\n",
      "        Fake     0.6381    0.8658    0.7347     10000\n",
      "\n",
      "    accuracy                         0.6874     20000\n",
      "   macro avg     0.7147    0.6874    0.6771     20000\n",
      "weighted avg     0.7147    0.6874    0.6771     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.04 seconds for 100000 samples\n",
      "Prediction time: 2.56 seconds for 20000 samples\n",
      "Model size (joblib): 18.190 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7697    0.7871    0.7783     10000\n",
      "        Fake     0.7822    0.7645    0.7732     10000\n",
      "\n",
      "    accuracy                         0.7758     20000\n",
      "   macro avg     0.7759    0.7758    0.7758     20000\n",
      "weighted avg     0.7759    0.7758    0.7758     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 4.63 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.002 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7916    0.8393    0.8147     10000\n",
      "        Fake     0.8290    0.7790    0.8032     10000\n",
      "\n",
      "    accuracy                         0.8092     20000\n",
      "   macro avg     0.8103    0.8092    0.8090     20000\n",
      "weighted avg     0.8103    0.8092    0.8090     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 4.80 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 14.112 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9185    0.9191    0.9188     10000\n",
      "        Fake     0.9191    0.9185    0.9188     10000\n",
      "\n",
      "    accuracy                         0.9188     20000\n",
      "   macro avg     0.9188    0.9188    0.9188     20000\n",
      "weighted avg     0.9188    0.9188    0.9188     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 1.26 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.141 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9290    0.9225    0.9257     10000\n",
      "        Fake     0.9230    0.9295    0.9263     10000\n",
      "\n",
      "    accuracy                         0.9260     20000\n",
      "   macro avg     0.9260    0.9260    0.9260     20000\n",
      "weighted avg     0.9260    0.9260    0.9260     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noLBP\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.34 seconds for 100000 samples\n",
      "Prediction time: 0.13 seconds for 20000 samples\n",
      "Model size (joblib): 0.009 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8106    0.6763    0.7374     10000\n",
      "        Fake     0.7223    0.8420    0.7776     10000\n",
      "\n",
      "    accuracy                         0.7591     20000\n",
      "   macro avg     0.7665    0.7591    0.7575     20000\n",
      "weighted avg     0.7665    0.7591    0.7575     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 6.10 seconds for 20000 samples\n",
      "Model size (joblib): 129.623 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7550    0.7795    0.7670     10000\n",
      "        Fake     0.7721    0.7470    0.7593     10000\n",
      "\n",
      "    accuracy                         0.7632     20000\n",
      "   macro avg     0.7635    0.7632    0.7632     20000\n",
      "weighted avg     0.7635    0.7632    0.7632     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 5.99 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.005 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8383    0.8541    0.8461     10000\n",
      "        Fake     0.8513    0.8353    0.8432     10000\n",
      "\n",
      "    accuracy                         0.8447     20000\n",
      "   macro avg     0.8448    0.8447    0.8447     20000\n",
      "weighted avg     0.8448    0.8447    0.8447     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.65 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 13.960 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9058    0.9149    0.9103     10000\n",
      "        Fake     0.9140    0.9049    0.9094     10000\n",
      "\n",
      "    accuracy                         0.9099     20000\n",
      "   macro avg     0.9099    0.9099    0.9099     20000\n",
      "weighted avg     0.9099    0.9099    0.9099     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 4.31 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.148 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9300    0.9276    0.9288     10000\n",
      "        Fake     0.9278    0.9302    0.9290     10000\n",
      "\n",
      "    accuracy                         0.9289     20000\n",
      "   macro avg     0.9289    0.9289    0.9289     20000\n",
      "weighted avg     0.9289    0.9289    0.9289     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noHOG\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8035    0.5254    0.6353     10000\n",
      "        Fake     0.6474    0.8715    0.7429     10000\n",
      "\n",
      "    accuracy                         0.6985     20000\n",
      "   macro avg     0.7255    0.6985    0.6891     20000\n",
      "weighted avg     0.7255    0.6985    0.6891     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.04 seconds for 100000 samples\n",
      "Prediction time: 2.73 seconds for 20000 samples\n",
      "Model size (joblib): 19.680 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8002    0.8283    0.8140     10000\n",
      "        Fake     0.8221    0.7932    0.8074     10000\n",
      "\n",
      "    accuracy                         0.8107     20000\n",
      "   macro avg     0.8111    0.8108    0.8107     20000\n",
      "weighted avg     0.8111    0.8107    0.8107     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 4.57 seconds for 100000 samples\n",
      "Prediction time: 0.01 seconds for 20000 samples\n",
      "Model size (joblib): 0.003 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8316    0.8679    0.8494     10000\n",
      "        Fake     0.8619    0.8243    0.8427     10000\n",
      "\n",
      "    accuracy                         0.8461     20000\n",
      "   macro avg     0.8468    0.8461    0.8460     20000\n",
      "weighted avg     0.8468    0.8461    0.8460     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 5.30 seconds for 100000 samples\n",
      "Prediction time: 0.05 seconds for 20000 samples\n",
      "Model size (joblib): 13.267 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9256    0.9293    0.9274     10000\n",
      "        Fake     0.9290    0.9253    0.9272     10000\n",
      "\n",
      "    accuracy                         0.9273     20000\n",
      "   macro avg     0.9273    0.9273    0.9273     20000\n",
      "weighted avg     0.9273    0.9273    0.9273     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 1.31 seconds for 100000 samples\n",
      "Prediction time: 0.02 seconds for 20000 samples\n",
      "Model size (joblib): 0.140 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9324    0.9300    0.9312     10000\n",
      "        Fake     0.9302    0.9326    0.9314     10000\n",
      "\n",
      "    accuracy                         0.9313     20000\n",
      "   macro avg     0.9313    0.9313    0.9313     20000\n",
      "weighted avg     0.9313    0.9313    0.9313     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noGabor\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.30 seconds for 100000 samples\n",
      "Prediction time: 0.12 seconds for 20000 samples\n",
      "Model size (joblib): 0.009 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7942    0.6666    0.7248     10000\n",
      "        Fake     0.7128    0.8273    0.7658     10000\n",
      "\n",
      "    accuracy                         0.7470     20000\n",
      "   macro avg     0.7535    0.7470    0.7453     20000\n",
      "weighted avg     0.7535    0.7470    0.7453     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.12 seconds for 100000 samples\n",
      "Prediction time: 6.08 seconds for 20000 samples\n",
      "Model size (joblib): 125.477 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7561    0.7808    0.7683     10000\n",
      "        Fake     0.7734    0.7482    0.7606     10000\n",
      "\n",
      "    accuracy                         0.7645     20000\n",
      "   macro avg     0.7648    0.7645    0.7644     20000\n",
      "weighted avg     0.7648    0.7645    0.7644     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 8.21 seconds for 100000 samples\n",
      "Prediction time: 0.04 seconds for 20000 samples\n",
      "Model size (joblib): 0.005 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8295    0.8552    0.8421     10000\n",
      "        Fake     0.8506    0.8242    0.8372     10000\n",
      "\n",
      "    accuracy                         0.8397     20000\n",
      "   macro avg     0.8400    0.8397    0.8397     20000\n",
      "weighted avg     0.8400    0.8397    0.8397     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 18.49 seconds for 100000 samples\n",
      "Prediction time: 0.09 seconds for 20000 samples\n",
      "Model size (joblib): 15.324 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8709    0.8888    0.8797     10000\n",
      "        Fake     0.8865    0.8682    0.8772     10000\n",
      "\n",
      "    accuracy                         0.8785     20000\n",
      "   macro avg     0.8787    0.8785    0.8785     20000\n",
      "weighted avg     0.8787    0.8785    0.8785     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 4.42 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.146 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8871    0.8957    0.8914     10000\n",
      "        Fake     0.8947    0.8860    0.8903     10000\n",
      "\n",
      "    accuracy                         0.8909     20000\n",
      "   macro avg     0.8909    0.8909    0.8908     20000\n",
      "weighted avg     0.8909    0.8909    0.8908     20000\n",
      "\n",
      "==================================================\n",
      "Running experiments for feature set: All_noColor\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB:\n",
      "Training time: 0.21 seconds for 100000 samples\n",
      "Prediction time: 0.09 seconds for 20000 samples\n",
      "Model size (joblib): 0.006 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.6797    0.6983    0.6889     10000\n",
      "        Fake     0.6898    0.6710    0.6803     10000\n",
      "\n",
      "    accuracy                         0.6846     20000\n",
      "   macro avg     0.6848    0.6846    0.6846     20000\n",
      "weighted avg     0.6848    0.6846    0.6846     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier:\n",
      "Training time: 0.08 seconds for 100000 samples\n",
      "Prediction time: 5.14 seconds for 20000 samples\n",
      "Model size (joblib): 115.929 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7357    0.7740    0.7543     10000\n",
      "        Fake     0.7616    0.7219    0.7412     10000\n",
      "\n",
      "    accuracy                         0.7480     20000\n",
      "   macro avg     0.7486    0.7480    0.7478     20000\n",
      "weighted avg     0.7486    0.7480    0.7478     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression:\n",
      "Training time: 9.06 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.004 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.7957    0.8068    0.8012     10000\n",
      "        Fake     0.8041    0.7929    0.7984     10000\n",
      "\n",
      "    accuracy                         0.7998     20000\n",
      "   macro avg     0.7999    0.7998    0.7998     20000\n",
      "weighted avg     0.7999    0.7998    0.7998     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier:\n",
      "Training time: 20.56 seconds for 100000 samples\n",
      "Prediction time: 0.07 seconds for 20000 samples\n",
      "Model size (joblib): 16.247 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8542    0.8619    0.8580     10000\n",
      "        Fake     0.8606    0.8529    0.8568     10000\n",
      "\n",
      "    accuracy                         0.8574     20000\n",
      "   macro avg     0.8574    0.8574    0.8574     20000\n",
      "weighted avg     0.8574    0.8574    0.8574     20000\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LGBMClassifier:\n",
      "Training time: 2.98 seconds for 100000 samples\n",
      "Prediction time: 0.03 seconds for 20000 samples\n",
      "Model size (joblib): 0.149 MB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.8935    0.8905    0.8920     10000\n",
      "        Fake     0.8909    0.8939    0.8924     10000\n",
      "\n",
      "    accuracy                         0.8922     20000\n",
      "   macro avg     0.8922    0.8922    0.8922     20000\n",
      "weighted avg     0.8922    0.8922    0.8922     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature_set_name, feature_cols in FEATURE_SETS_ABLATION.items():\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Running experiments for feature set: {feature_set_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    X_train = df_all_train[feature_cols]\n",
    "    X_test = df_all_test[feature_cols]\n",
    "    y_train = df_all_train[\"label\"]\n",
    "    y_test = df_all_test[\"label\"]\n",
    "    X_train, y_train = shuffle_indexes(X_train, y_train)\n",
    "    run_ml_experiments(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        path_output=Path(\"outputs/\"),\n",
    "        feature_set_name=feature_set_name,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "at82-08-ai-generated-image-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
